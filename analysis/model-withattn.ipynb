{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "import _pickle as pk\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "import copy, warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'num_train': 20000,\n",
    "    'num_valid': 5000,\n",
    "    'pred_maxlen': 10,\n",
    "    \n",
    "    'lr': 0.001,\n",
    "    'mt': 0.99,\n",
    "    'emb_dim': 64,\n",
    "    'lstm_size': 128,\n",
    "    'attn_size': 100,\n",
    "    'dropout': 0.5,\n",
    "    'batch': 32,\n",
    "    'epoch': 50,\n",
    "    \n",
    "    'testfile': \"test.txt\",\n",
    "    'lossfile': 'model.loss',\n",
    "    'checkpoint': \"model.pt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_letters_dataset(size):\n",
    "    dataset = []\n",
    "    for _ in range(size):\n",
    "        x = []\n",
    "        for _ in range(random.randint(3, 10)):\n",
    "            letter = chr(random.randint(97, 122))\n",
    "            repeat = [letter] * random.randint(1, 3)\n",
    "            x.extend(repeat)\n",
    "        y = sorted(set(x))\n",
    "        dataset.append((x, y))\n",
    "    return zip(*dataset)\n",
    "\n",
    "train_inp, train_out = sorting_letters_dataset(config['num_train'])\n",
    "valid_inp, valid_out = sorting_letters_dataset(config['num_valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the training dataset: 20000\n",
      "Length of the validation dataset: 5000\n"
     ]
    }
   ],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, vocab):\n",
    "        self.itos = vocab\n",
    "        self.stoi = {d:i for i, d in enumerate(self.itos)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos) \n",
    "\n",
    "src_vocab = Vocab(['<pad>'] + [chr(i+97) for i in range(26)])\n",
    "tgt_vocab = Vocab(['<pad>'] + [chr(i+97) for i in range(26)] + ['<start>', '<stop>'] )\n",
    "\n",
    "def map_elems(elems, mapper):\n",
    "    return [mapper[elem] for elem in elems]\n",
    "\n",
    "def map_many_elems(many_elems, mapper):\n",
    "    return [map_elems(elems, mapper) for elems in many_elems]\n",
    "\n",
    "train_x = map_many_elems(train_inp, src_vocab.stoi)\n",
    "train_y = map_many_elems(train_out, tgt_vocab.stoi)\n",
    "print(\"Length of the training dataset: {}\".format(len(train_x)))\n",
    "\n",
    "valid_x = map_many_elems(valid_inp, src_vocab.stoi)\n",
    "valid_y = map_many_elems(valid_out, tgt_vocab.stoi)\n",
    "print(\"Length of the validation dataset: {}\".format(len(valid_x)))\n",
    "\n",
    "PAD_IX   = tgt_vocab.stoi['<pad>']\n",
    "START_IX = tgt_vocab.stoi['<start>']\n",
    "STOP_IX  = tgt_vocab.stoi['<stop>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (emb): Embedding(27, 64)\n",
      "  (lstm): LSTM(64, 128, batch_first=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, lstm_size, z_type, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.z_index = z_type\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, lstm_size, batch_first=True)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, enc_inputs):\n",
    "        batch_inputs = copy.deepcopy(enc_inputs)\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        x_tensor = [torch.tensor(sample).to(device) for sample in batch_inputs]\n",
    "        x_pad = pad_sequence(x_tensor, batch_first=True, padding_value=PAD_IX) # (batch, seqlen) \n",
    "        x_emb = self.emb(x_pad) # (batch, seqlen, emb_dim) \n",
    "        x_emb = self.drop(x_emb)\n",
    "        \n",
    "        x_len = [len(sample) for sample in batch_inputs]\n",
    "        x_pack = pack_padded_sequence(x_emb, x_len, batch_first=True, enforce_sorted=False)\n",
    "        outs_pack, (h_n, c_n) = self.lstm(x_pack)\n",
    "        outs, _ = pad_packed_sequence(outs_pack, batch_first=True)\n",
    "            \n",
    "        if self.z_index == 1:\n",
    "            return h_n[0], c_n[0] # (seqlen, batch, lstm_dim)\n",
    "        else:\n",
    "            return outs # (batch, seqlen, lstm_dim)\n",
    "\n",
    "encoder = Encoder(vocab_size=len(src_vocab), \n",
    "                  emb_dim=config['emb_dim'], \n",
    "                  lstm_size=config['lstm_size'], \n",
    "                  z_type=0)\n",
    "\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionDecoder(\n",
      "  (emb): Embedding(29, 64)\n",
      "  (lstm): LSTMCell(64, 128)\n",
      "  (attn): Attention(\n",
      "    (W): Linear(in_features=256, out_features=100, bias=True)\n",
      "    (v): Linear(in_features=100, out_features=1, bias=False)\n",
      "  )\n",
      "  (clf): Linear(in_features=256, out_features=29, bias=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (objective): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, attn_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, attn_dim)\n",
    "        self.v = nn.Linear(attn_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, dec_hidden, enc_outs):\n",
    "        # enc_outs -> (batch, seqlen, hidden)\n",
    "        # dec_hidden -> (batch, hidden)\n",
    "        \n",
    "        seqlen = enc_outs.size(1)\n",
    "        \n",
    "        repeat_h = dec_hidden.unsqueeze(1)  # make room to repeat on seqlen dim\n",
    "        repeat_h = repeat_h.repeat(1, seqlen, 1)  # (1, seqlen, hidden)\n",
    "\n",
    "        concat_h = torch.cat((enc_outs, repeat_h), dim=2) # (1, seqlen, hidden*2)\n",
    "        \n",
    "        scores = self.v(torch.tanh(self.W(concat_h))) # (1, seqlen, 1)\n",
    "        probs = torch.softmax(scores, dim=1)\n",
    "        \n",
    "        weighted = enc_outs * probs # (1, seqlen, hidden)\n",
    "        \n",
    "        context = torch.sum(weighted, dim=1, keepdim=False) # (1, hidden)\n",
    "        combined = torch.cat((dec_hidden, context), dim=1)  # (1, hidden*2)\n",
    "        \n",
    "        return combined\n",
    "\n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, lstm_size, attn_size, dropout=0.5):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        \n",
    "        self.lstm_size = lstm_size\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTMCell(emb_dim, lstm_size)\n",
    "        self.attn = Attention(lstm_size * 2, attn_size)\n",
    "        self.clf = nn.Linear(lstm_size * 2, vocab_size)\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.objective = nn.CrossEntropyLoss(reduction=\"none\", ignore_index=PAD_IX)\n",
    "        \n",
    "    def init_state(self, batch_size, device):\n",
    "        h_0 = torch.zeros(batch_size, self.lstm_size).to(device)  # (batch, hidden_size)\n",
    "        c_0 = torch.zeros(batch_size, self.lstm_size).to(device)  # (batch, hidden_size)\n",
    "        return h_0, c_0\n",
    "    \n",
    "    def pad_targets(self, targets):\n",
    "        maxlen = max([len(target) for target in targets])\n",
    "        for i in range(len(targets)): \n",
    "            targets[i].append(STOP_IX) #added last token\n",
    "            targets[i].extend([PAD_IX] * (maxlen + 1 - len(targets[i]))) #added pad token\n",
    "        return targets, maxlen\n",
    "            \n",
    "    def forward(self, enc_outs, dec_targets, curr_token, last_token):\n",
    "        batch_targets = copy.deepcopy(dec_targets)\n",
    "        device = next(self.parameters()).device\n",
    "        batch_size = enc_outs.shape[0]\n",
    "        state = self.init_state(batch_size, device) # (batch, lstm_dim)\n",
    "        \n",
    "        batch_targets, maxlen = self.pad_targets(batch_targets)\n",
    "        \n",
    "        batch_loss = 0.0\n",
    "        curr_tokens = [curr_token] * batch_size\n",
    "        for i in range(maxlen + 1):\n",
    "            inputs = torch.tensor(curr_tokens).to(device) # (batch)\n",
    "            \n",
    "            emb = self.emb(inputs) # (batch, emb_dim)\n",
    "            emb = self.drop(emb) # (batch, emb_dim)\n",
    "            \n",
    "            state = self.lstm(emb, state) # (batch, lstm_dim)\n",
    "            q_i, _ = state \n",
    "            q_i = self.drop(q_i) # (batch, lstm_dim)\n",
    "            \n",
    "            combined = self.attn(q_i, enc_outs) # (batch, lstm_dim * 2)\n",
    "            scores = self.clf(combined) # (batch, tgt_vocab)\n",
    "            \n",
    "            next_tokens = [targets[i] for targets in batch_targets]\n",
    "            targets = torch.tensor(next_tokens).to(device) # (batch)\n",
    "            batch_loss += self.objective(scores, targets) # (batch)\n",
    "            \n",
    "            curr_tokens = next_tokens\n",
    "        \n",
    "        maskcount = [np.count_nonzero(targets) for targets in batch_targets]\n",
    "        maskcount = torch.tensor(maskcount, dtype=torch.float32).to(device)\n",
    "        batch_loss = (batch_loss/maskcount).sum()\n",
    "        \n",
    "        return batch_loss\n",
    "    \n",
    "    def predict(self, enc_outs, curr_token, last_token, maxlen):\n",
    "        device = next(self.parameters()).device\n",
    "        batch_size = enc_outs.shape[0]\n",
    "        state = self.init_state(batch_size, device) # (batch, lstm_dim)\n",
    "        \n",
    "        batch_preds = []\n",
    "        curr_tokens = [curr_token] * batch_size\n",
    "        for i in range(maxlen + 1):\n",
    "            inputs = torch.tensor(curr_tokens).to(device) # (batch)\n",
    "            \n",
    "            emb = self.emb(inputs) # (batch, emb_dim)\n",
    "            \n",
    "            state = self.lstm(emb, state) # (batch, lstm_dim)\n",
    "            q_i, _ = state \n",
    "            \n",
    "            combined = self.attn(q_i, enc_outs) # (batch, lstm_dim * 2)\n",
    "            scores = self.clf(combined) # (batch, tgt_vocab)\n",
    "            \n",
    "            pred_tokens = torch.argmax(torch.softmax(scores, dim=1), dim=1) # (batch)\n",
    "            curr_tokens = pred_tokens\n",
    "            batch_preds.append(pred_tokens.tolist())\n",
    "\n",
    "        batch_preds = np.array(batch_preds).T.tolist()\n",
    "        for ix, _ in enumerate(batch_preds):\n",
    "            if last_token in batch_preds[ix]:\n",
    "                last_token_ix = batch_preds[ix].index(last_token)\n",
    "                batch_preds[ix] = batch_preds[ix][:last_token_ix]\n",
    "        return batch_preds\n",
    "    \n",
    "    def evaluate(self, enc_outs, dec_targets, curr_token, last_token):\n",
    "        batch_targets = copy.deepcopy(dec_targets)\n",
    "        device = next(self.parameters()).device\n",
    "        batch_size = enc_outs.shape[0]\n",
    "        state = self.init_state(batch_size, device) # (batch, lstm_dim)\n",
    "        \n",
    "        batch_targets, maxlen = self.pad_targets(batch_targets)\n",
    "        \n",
    "        batch_preds, batch_loss = [], 0.0\n",
    "        curr_tokens = [curr_token] * batch_size\n",
    "        for i in range(maxlen + 1):\n",
    "            inputs = torch.tensor(curr_tokens).to(device) # (batch)\n",
    "            \n",
    "            emb = self.emb(inputs) # (batch, emb_dim)\n",
    "            \n",
    "            state = self.lstm(emb, state) # (batch, lstm_dim)\n",
    "            q_i, _ = state\n",
    "            \n",
    "            combined = self.attn(q_i, enc_outs) # (batch, lstm_dim * 2)\n",
    "            scores = self.clf(combined) # (batch, tgt_vocab)\n",
    "            \n",
    "            next_tokens = [targets[i] for targets in batch_targets]\n",
    "            targets = torch.tensor(next_tokens).to(device) # (batch)\n",
    "            batch_loss += self.objective(scores, targets) # (batch)\n",
    "            \n",
    "            pred_tokens = torch.argmax(torch.softmax(scores, dim=1), dim=1) # (batch)\n",
    "            curr_tokens = pred_tokens\n",
    "            batch_preds.append(pred_tokens.tolist())\n",
    "        \n",
    "        maskcount = [np.count_nonzero(targets) for targets in batch_targets]\n",
    "        maskcount = torch.tensor(maskcount, dtype=torch.float32).to(device)\n",
    "        batch_loss = (batch_loss/maskcount).sum()\n",
    "        \n",
    "        batch_preds = np.array(batch_preds).T.tolist()\n",
    "        for ix, _ in enumerate(batch_preds):\n",
    "            if last_token in batch_preds[ix]:\n",
    "                last_token_ix = batch_preds[ix].index(last_token)\n",
    "                batch_preds[ix] = batch_preds[ix][:last_token_ix]\n",
    "        \n",
    "        return batch_preds, batch_loss\n",
    "\n",
    "decoder = AttentionDecoder(vocab_size=len(tgt_vocab), \n",
    "                           emb_dim=config['emb_dim'], \n",
    "                           lstm_size=config['lstm_size'], \n",
    "                           attn_size=config['attn_size'])\n",
    "\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_prediction(sample_preds):\n",
    "    sample_preds = [[tgt_vocab.itos[ix] for ix in each_preds] for each_preds in sample_preds]\n",
    "    sample_preds = [''.join(each_preds) for each_preds in sample_preds]\n",
    "    return sample_preds\n",
    "\n",
    "def shuffle(x, y):\n",
    "    pack = list(zip(x, y))\n",
    "    random.shuffle(pack)\n",
    "    return zip(*pack)\n",
    "\n",
    "def evaluate(encoder, decoder, sample_x, sample_y, batch_size):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    sample_loss = 0.0\n",
    "    batch_x, batch_y = [], []\n",
    "    predictions, actuals = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(sample_x)):\n",
    "            batch_x.append(sample_x[i])\n",
    "            batch_y.append(sample_y[i])\n",
    "            \n",
    "            if len(batch_x) == batch_size or i == len(sample_x) - 1:\n",
    "                batch_preds, batch_loss = decoder.evaluate(encoder(batch_x), batch_y, START_IX, STOP_IX)\n",
    "                \n",
    "                batch_preds = map_prediction(batch_preds)\n",
    "                predictions.extend(batch_preds)\n",
    "                batch_y = map_prediction(batch_y)\n",
    "                actuals.extend(batch_y)\n",
    "                \n",
    "                sample_loss += batch_loss.item()\n",
    "                batch_x, batch_y = [], []\n",
    "    \n",
    "    sample_loss = sample_loss / len(sample_x) * 1.0\n",
    "    \n",
    "    accuracy = accuracy_score(actuals, predictions)\n",
    "    return predictions, sample_loss, accuracy\n",
    "\n",
    "def train(encoder, enc_optim, decoder, dec_optim, train_x, train_y, batch_size):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    train_x, train_y = shuffle(train_x, train_y)\n",
    "    batch_x, batch_y = [], []\n",
    "\n",
    "    for i in range(len(train_x)):\n",
    "        batch_x.append(train_x[i])\n",
    "        batch_y.append(train_y[i])\n",
    "\n",
    "        if len(batch_x) == batch_size or i == len(train_x) - 1:\n",
    "            encoder.zero_grad(); enc_optim.zero_grad()\n",
    "            decoder.zero_grad(); dec_optim.zero_grad()\n",
    "        \n",
    "            batch_loss = decoder(encoder(batch_x), batch_y, START_IX, STOP_IX)\n",
    "\n",
    "            batch_loss.backward()\n",
    "            enc_optim.step()\n",
    "            dec_optim.step()\n",
    "\n",
    "            train_loss += batch_loss.item()\n",
    "            batch_x, batch_y = [], []\n",
    "\n",
    "    train_loss = train_loss / len(train_x) * 1.0\n",
    "    \n",
    "    return encoder, decoder, train_x, train_y, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCurrentTime():\n",
    "    return str(datetime.datetime.now())\n",
    "\n",
    "def track_best_model(encoder, decoder, epoch, best_acc, valid_acc, valid_loss):\n",
    "    if best_acc >= valid_acc:\n",
    "        return best_acc, ''\n",
    "    state = {\n",
    "        'encoder': encoder.state_dict(), \n",
    "        'decoder': decoder.state_dict(),\n",
    "        'acc': valid_acc,\n",
    "        'loss': valid_loss,\n",
    "        'epoch': epoch\n",
    "    }\n",
    "    torch.save(state, config['checkpoint'])\n",
    "    return valid_acc, ' * '\n",
    "\n",
    "def training_loop(encoder, decoder, train_x, train_y, epochs, batch_size, print_every=1):\n",
    "    print(\"\\nTraining with encoder and decoder...\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"\\nAttaching to device: {}\".format(device))\n",
    "    \n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    enc_optim = optim.SGD(encoder.parameters(), lr=config['lr'], momentum=config['mt'])\n",
    "    dec_optim = optim.SGD(decoder.parameters(), lr=config['lr'], momentum=config['mt'])\n",
    "    \n",
    "    best_acc = -1.0\n",
    "    keep_loss = [[], []] # [[train],[valid]]\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        encoder, decoder, train_x, train_y, train_loss = train(encoder, enc_optim, decoder, dec_optim, train_x, train_y, batch_size)\n",
    "        _, valid_loss, valid_acc = evaluate(encoder, decoder, valid_x, valid_y, batch_size)\n",
    "        best_acc, epoch_track = track_best_model(encoder, decoder, epoch, best_acc, valid_acc, valid_loss)\n",
    "\n",
    "        \n",
    "        keep_loss[0].append(train_loss)\n",
    "        keep_loss[1].append(valid_loss)\n",
    "        \n",
    "        epoch_msg = '\\n[{}] Epoch {:02d}: [TRAIN] Loss: {:.6f}'.format(getCurrentTime(), epoch, train_loss)\n",
    "        epoch_msg += ' [VAL] Loss: {:.6f}, Acc: {:.6f}'.format(valid_loss, valid_acc)\n",
    "        print(epoch_msg + epoch_track)\n",
    "    \n",
    "    with open(config['lossfile'], 'wb') as lossfile:\n",
    "        pk.dump(keep_loss, lossfile)\n",
    "    \n",
    "    print(\"\\nTraining completed...\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with encoder and decoder...\n",
      "\n",
      "Attaching to device: cuda\n",
      "\n",
      "[2020-04-11 16:26:35.167664] Epoch 01: [TRAIN] Loss: 1.326269 [VAL] Loss: 1.308529, Acc: 0.734600 * \n",
      "\n",
      "[2020-04-11 16:26:49.735896] Epoch 02: [TRAIN] Loss: 0.277904 [VAL] Loss: 0.377637, Acc: 0.935000 * \n",
      "\n",
      "[2020-04-11 16:27:04.244988] Epoch 03: [TRAIN] Loss: 0.154015 [VAL] Loss: 0.202143, Acc: 0.969000 * \n",
      "\n",
      "[2020-04-11 16:27:18.818182] Epoch 04: [TRAIN] Loss: 0.107612 [VAL] Loss: 0.149917, Acc: 0.976800 * \n",
      "\n",
      "[2020-04-11 16:27:33.322391] Epoch 05: [TRAIN] Loss: 0.086642 [VAL] Loss: 0.083299, Acc: 0.987800 * \n",
      "\n",
      "[2020-04-11 16:27:47.822166] Epoch 06: [TRAIN] Loss: 0.082515 [VAL] Loss: 0.109729, Acc: 0.983000\n",
      "\n",
      "[2020-04-11 16:28:02.401725] Epoch 07: [TRAIN] Loss: 0.086080 [VAL] Loss: 0.140087, Acc: 0.978600\n",
      "\n",
      "[2020-04-11 16:28:16.896313] Epoch 08: [TRAIN] Loss: 0.074891 [VAL] Loss: 0.065171, Acc: 0.991400 * \n",
      "\n",
      "[2020-04-11 16:28:31.468322] Epoch 09: [TRAIN] Loss: 0.063610 [VAL] Loss: 0.054420, Acc: 0.992800 * \n",
      "\n",
      "[2020-04-11 16:28:45.977070] Epoch 10: [TRAIN] Loss: 0.060919 [VAL] Loss: 0.044879, Acc: 0.994400 * \n",
      "\n",
      "[2020-04-11 16:29:00.478472] Epoch 11: [TRAIN] Loss: 0.055709 [VAL] Loss: 0.069564, Acc: 0.992600\n",
      "\n",
      "[2020-04-11 16:29:15.055772] Epoch 12: [TRAIN] Loss: 0.053759 [VAL] Loss: 0.044022, Acc: 0.993600\n",
      "\n",
      "[2020-04-11 16:29:29.546109] Epoch 13: [TRAIN] Loss: 0.052262 [VAL] Loss: 0.055785, Acc: 0.992000\n",
      "\n",
      "[2020-04-11 16:29:44.102149] Epoch 14: [TRAIN] Loss: 0.046426 [VAL] Loss: 0.038919, Acc: 0.996600 * \n",
      "\n",
      "[2020-04-11 16:29:58.608590] Epoch 15: [TRAIN] Loss: 0.045396 [VAL] Loss: 0.022515, Acc: 0.996400\n",
      "\n",
      "[2020-04-11 16:30:13.204810] Epoch 16: [TRAIN] Loss: 0.044725 [VAL] Loss: 0.049286, Acc: 0.993800\n",
      "\n",
      "[2020-04-11 16:30:27.723505] Epoch 17: [TRAIN] Loss: 0.043180 [VAL] Loss: 0.047444, Acc: 0.995400\n",
      "\n",
      "[2020-04-11 16:30:42.235462] Epoch 18: [TRAIN] Loss: 0.043407 [VAL] Loss: 0.022144, Acc: 0.997200 * \n",
      "\n",
      "[2020-04-11 16:30:56.816526] Epoch 19: [TRAIN] Loss: 0.038683 [VAL] Loss: 0.032425, Acc: 0.996400\n",
      "\n",
      "[2020-04-11 16:31:11.346106] Epoch 20: [TRAIN] Loss: 0.041567 [VAL] Loss: 0.030613, Acc: 0.996000\n",
      "\n",
      "[2020-04-11 16:31:25.905513] Epoch 21: [TRAIN] Loss: 0.042833 [VAL] Loss: 0.040503, Acc: 0.996000\n",
      "\n",
      "[2020-04-11 16:31:40.309486] Epoch 22: [TRAIN] Loss: 0.039456 [VAL] Loss: 0.030600, Acc: 0.996600\n",
      "\n",
      "[2020-04-11 16:31:54.710544] Epoch 23: [TRAIN] Loss: 0.040326 [VAL] Loss: 0.032159, Acc: 0.995000\n",
      "\n",
      "[2020-04-11 16:32:09.221633] Epoch 24: [TRAIN] Loss: 0.043830 [VAL] Loss: 0.028464, Acc: 0.994600\n",
      "\n",
      "[2020-04-11 16:32:23.653690] Epoch 25: [TRAIN] Loss: 0.042994 [VAL] Loss: 0.052393, Acc: 0.994800\n",
      "\n",
      "[2020-04-11 16:32:38.122178] Epoch 26: [TRAIN] Loss: 0.042672 [VAL] Loss: 0.056151, Acc: 0.993400\n",
      "\n",
      "[2020-04-11 16:32:52.528638] Epoch 27: [TRAIN] Loss: 0.039462 [VAL] Loss: 0.029573, Acc: 0.996600\n",
      "\n",
      "[2020-04-11 16:33:06.964749] Epoch 28: [TRAIN] Loss: 0.040575 [VAL] Loss: 0.014874, Acc: 0.998400 * \n",
      "\n",
      "[2020-04-11 16:33:21.452145] Epoch 29: [TRAIN] Loss: 0.041296 [VAL] Loss: 0.029702, Acc: 0.996400\n",
      "\n",
      "[2020-04-11 16:33:35.876851] Epoch 30: [TRAIN] Loss: 0.038535 [VAL] Loss: 0.017465, Acc: 0.997800\n",
      "\n",
      "[2020-04-11 16:33:50.372357] Epoch 31: [TRAIN] Loss: 0.036948 [VAL] Loss: 0.011558, Acc: 0.999000 * \n",
      "\n",
      "[2020-04-11 16:34:04.792159] Epoch 32: [TRAIN] Loss: 0.039509 [VAL] Loss: 0.058063, Acc: 0.995400\n",
      "\n",
      "[2020-04-11 16:34:19.215176] Epoch 33: [TRAIN] Loss: 0.041468 [VAL] Loss: 0.018154, Acc: 0.998000\n",
      "\n",
      "[2020-04-11 16:34:33.702649] Epoch 34: [TRAIN] Loss: 0.047692 [VAL] Loss: 0.007775, Acc: 0.999000\n",
      "\n",
      "[2020-04-11 16:34:48.129144] Epoch 35: [TRAIN] Loss: 0.043887 [VAL] Loss: 0.030024, Acc: 0.996400\n",
      "\n",
      "[2020-04-11 16:35:02.614443] Epoch 36: [TRAIN] Loss: 0.042824 [VAL] Loss: 0.025703, Acc: 0.995600\n",
      "\n",
      "[2020-04-11 16:35:17.005056] Epoch 37: [TRAIN] Loss: 0.045349 [VAL] Loss: 0.020100, Acc: 0.998200\n",
      "\n",
      "[2020-04-11 16:35:31.438151] Epoch 38: [TRAIN] Loss: 0.045571 [VAL] Loss: 0.015790, Acc: 0.997800\n",
      "\n",
      "[2020-04-11 16:35:45.927498] Epoch 39: [TRAIN] Loss: 0.044526 [VAL] Loss: 0.022560, Acc: 0.998000\n",
      "\n",
      "[2020-04-11 16:36:00.354432] Epoch 40: [TRAIN] Loss: 0.045641 [VAL] Loss: 0.021431, Acc: 0.997600\n",
      "\n",
      "[2020-04-11 16:36:14.841999] Epoch 41: [TRAIN] Loss: 0.051122 [VAL] Loss: 0.018581, Acc: 0.998400\n",
      "\n",
      "[2020-04-11 16:36:29.260982] Epoch 42: [TRAIN] Loss: 0.048637 [VAL] Loss: 0.040520, Acc: 0.995800\n",
      "\n",
      "[2020-04-11 16:36:43.676177] Epoch 43: [TRAIN] Loss: 0.042668 [VAL] Loss: 0.017035, Acc: 0.998400\n",
      "\n",
      "[2020-04-11 16:36:58.157490] Epoch 44: [TRAIN] Loss: 0.041984 [VAL] Loss: 0.014107, Acc: 0.998800\n",
      "\n",
      "[2020-04-11 16:37:12.572507] Epoch 45: [TRAIN] Loss: 0.047140 [VAL] Loss: 0.018522, Acc: 0.998000\n",
      "\n",
      "[2020-04-11 16:37:27.049790] Epoch 46: [TRAIN] Loss: 0.047383 [VAL] Loss: 0.010966, Acc: 0.999200 * \n",
      "\n",
      "[2020-04-11 16:37:41.459995] Epoch 47: [TRAIN] Loss: 0.041236 [VAL] Loss: 0.031945, Acc: 0.997000\n",
      "\n",
      "[2020-04-11 16:37:55.978579] Epoch 48: [TRAIN] Loss: 0.038749 [VAL] Loss: 0.006929, Acc: 0.999000\n",
      "\n",
      "[2020-04-11 16:38:10.399940] Epoch 49: [TRAIN] Loss: 0.042980 [VAL] Loss: 0.007858, Acc: 0.999200\n",
      "\n",
      "[2020-04-11 16:38:24.843154] Epoch 50: [TRAIN] Loss: 0.041912 [VAL] Loss: 0.021220, Acc: 0.997400\n",
      "\n",
      "Training completed...\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(config['checkpoint']):\n",
    "    training_loop(encoder, decoder, train_x, train_y, config['epoch'], config['batch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving best model from epoch 46 with [DEV] loss 0.010966 and accuracy 0.999200.\n"
     ]
    }
   ],
   "source": [
    "def load_best_model():\n",
    "    encoder = Encoder(vocab_size=len(src_vocab), \n",
    "                  emb_dim=config['emb_dim'], \n",
    "                  lstm_size=config['lstm_size'], \n",
    "                  z_type=0)\n",
    "    decoder = AttentionDecoder(vocab_size=len(tgt_vocab), \n",
    "                               emb_dim=config['emb_dim'], \n",
    "                               lstm_size=config['lstm_size'], \n",
    "                               attn_size=config['attn_size'])\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    state = torch.load(config['checkpoint'], map_location=device)\n",
    "    encoder.load_state_dict(state['encoder'])\n",
    "    decoder.load_state_dict(state['decoder'])\n",
    "    state = {'acc': state['acc'], 'loss': state['loss'], 'epoch': state['epoch']}\n",
    "    return encoder, decoder, state\n",
    "\n",
    "keep_loss = [[], []] # [[train],[valid]]\n",
    "with open(config['lossfile'], 'rb') as lossfile:\n",
    "    keep_loss = pk.load(lossfile)\n",
    "encoder, decoder, state = load_best_model()\n",
    "print('Retrieving best model from epoch {} with [DEV] loss {:.6f} and accuracy {:.6f}.'.format(state['epoch'], state['loss'], state['acc'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = [i for i in range(1, len(keep_loss[0])+1)]\n",
    "plt.plot(epochs, keep_loss[0], 'b', label=\"training loss\")\n",
    "plt.plot(epochs, keep_loss[1], 'g', label=\"validation loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('cross-entropy loss')\n",
    "plt.title('The training and validation losses.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the test dataset: 5000\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(config['testfile'], delimiter='\\t', header=None, usecols=[0,1])\n",
    "test_inp, test_out = test_data[0], test_data[1]  \n",
    "\n",
    "test_x = map_many_elems(test_inp, src_vocab.stoi)\n",
    "test_y = map_many_elems(test_out, tgt_vocab.stoi)\n",
    "\n",
    "print(\"Length of the test dataset: {}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set (max prediction size = 10) = 97.74%\n",
      "Accuracy on the test set (max prediction size = 20) = 99.60%\n"
     ]
    }
   ],
   "source": [
    "def predict(encoder, decoder, sample_x, batch_size, pred_size):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    batch_x = []\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(sample_x)):\n",
    "            batch_x.append(sample_x[i])\n",
    "            \n",
    "            if len(batch_x) == batch_size or i == len(sample_x) - 1:\n",
    "                batch_preds = decoder.predict(encoder(batch_x), START_IX, STOP_IX, pred_size)\n",
    "                batch_preds = map_prediction(batch_preds)\n",
    "                predictions.extend(batch_preds)\n",
    "                batch_x = []\n",
    "            \n",
    "    return predictions\n",
    "\n",
    "def getAccuracyScore(encoder, decoder, sample_x, sample_out, batch_size, pred_size):\n",
    "    predictions = predict(encoder, decoder, sample_x, batch_size, pred_size)\n",
    "    groundtruth = [''.join(str_y) for str_y in sample_out]\n",
    "    acc = accuracy_score(groundtruth, predictions)\n",
    "    return acc\n",
    "\n",
    "config['pred_maxlen'] = 10\n",
    "test_acc = getAccuracyScore(encoder, decoder, test_x, test_out, config['batch'], config['pred_maxlen']) * 100\n",
    "print('Accuracy on the test set (max prediction size = {}) = {:.2f}%'.format(config['pred_maxlen'], test_acc))\n",
    "\n",
    "config['pred_maxlen'] = 20\n",
    "test_acc = getAccuracyScore(encoder, decoder, test_x, test_out, config['batch'], config['pred_maxlen']) * 100\n",
    "print('Accuracy on the test set (max prediction size = {}) = {:.2f}%'.format(config['pred_maxlen'], test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
