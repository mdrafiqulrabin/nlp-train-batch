{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_letters_dataset(size):\n",
    "    dataset = []\n",
    "    for _ in range(size):\n",
    "        x = []\n",
    "        for _ in range(random.randint(3, 10)):\n",
    "            letter = chr(random.randint(97, 122))\n",
    "            repeat = [letter] * random.randint(1, 3)\n",
    "            x.extend(repeat)\n",
    "        y = sorted(set(x))\n",
    "        dataset.append((x, y))\n",
    "    return zip(*dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, vocab):\n",
    "        self.itos = vocab\n",
    "        self.stoi = {d:i for i, d in enumerate(self.itos)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos) \n",
    "\n",
    "src_vocab = Vocab(['<pad>'] + [chr(i+97) for i in range(26)])\n",
    "tgt_vocab = Vocab(['<pad>'] + [chr(i+97) for i in range(26)] + ['<start>', '<stop>'] )\n",
    "\n",
    "train_inp, train_out = sorting_letters_dataset(200) #20_000\n",
    "valid_inp, valid_out = sorting_letters_dataset(50) #5_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the text data into numeric values\n",
    "\n",
    "def map_elems(elems, mapper):\n",
    "    return [mapper[elem] for elem in elems]\n",
    "\n",
    "def map_many_elems(many_elems, mapper):\n",
    "    return [map_elems(elems, mapper) for elems in many_elems]\n",
    "\n",
    "train_x = map_many_elems(train_inp, src_vocab.stoi)\n",
    "train_y = map_many_elems(train_out, tgt_vocab.stoi)\n",
    "\n",
    "valid_x = map_many_elems(valid_inp, src_vocab.stoi)\n",
    "valid_y = map_many_elems(valid_out, tgt_vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (emb): Embedding(27, 16)\n",
       "  (lstm): LSTM(16, 32, batch_first=True)\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, lstm_size, z_type, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.z_index = z_type\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, lstm_size, batch_first=True)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        seq = torch.tensor([inputs]).to(device) # (1, seqlen)\n",
    "        emb = self.emb(seq) # (1, seqlen, emb_dim)\n",
    "        emb = self.drop(emb) \n",
    "        \n",
    "        outs, (h_n, c_n) = self.lstm(emb)\n",
    "        \n",
    "        if self.z_index == 1:\n",
    "            return h_n[0], c_n[0] # (seqlen, lstm_dim)\n",
    "        else:\n",
    "            return outs # (1, seqlen, lstm_dim)\n",
    "\n",
    "encoder = Encoder(vocab_size=len(src_vocab), emb_dim=16, lstm_size=32, z_type=1) # 64 128\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (emb): Embedding(29, 16)\n",
       "  (lstm): LSTMCell(16, 32)\n",
       "  (clf): Linear(in_features=32, out_features=29, bias=True)\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (objective): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, lstm_size, dropout=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTMCell(emb_dim, lstm_size)\n",
    "        self.clf = nn.Linear(lstm_size, vocab_size)\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.objective = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        \n",
    "    def forward(self, state, targets, curr_token, last_token):\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        loss = 0\n",
    "        shifted = targets + [last_token]\n",
    "        for i in range(len(shifted)):\n",
    "            inp = torch.tensor([curr_token]).to(device)\n",
    "            \n",
    "            emb = self.emb(inp)\n",
    "            emb = self.drop(emb)\n",
    "            \n",
    "            state = self.lstm(emb, state)\n",
    "            q_i, _ = state \n",
    "            q_i = self.drop(q_i)\n",
    "\n",
    "            scores = self.clf(q_i)\n",
    "            target = torch.tensor([shifted[i]]).to(device)\n",
    "            loss += self.objective(scores, target)\n",
    "            \n",
    "            curr_token = shifted[i]\n",
    "            \n",
    "        return loss / len(shifted)\n",
    "\n",
    "    def predict(self, state, curr_token, last_token, maxlen):\n",
    "        device = next(self.parameters()).device\n",
    "        preds = []\n",
    "        for i in range(maxlen):\n",
    "            inp = torch.tensor([curr_token]).to(device)\n",
    "            emb = self.emb(inp)\n",
    "            \n",
    "            state = self.lstm(emb, state)\n",
    "            h_i, _ = state\n",
    "            \n",
    "            scores = self.clf(h_i)\n",
    "            pred = torch.argmax(torch.softmax(scores, dim=1))\n",
    "            curr_token = pred\n",
    "            \n",
    "            if last_token == pred:\n",
    "                break\n",
    "            preds.append(pred)\n",
    "        return preds\n",
    "    \n",
    "decoder = Decoder(vocab_size=len(tgt_vocab), emb_dim=16, lstm_size=32) # 64 128\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_IX = tgt_vocab.stoi['<start>']\n",
    "STOP_IX  = tgt_vocab.stoi['<stop>']\n",
    "\n",
    "def shuffle(x, y):\n",
    "    pack = list(zip(x, y))\n",
    "    random.shuffle(pack)\n",
    "    return zip(*pack)\n",
    "\n",
    "\n",
    "def train(encoder, decoder, train_x, train_y, batch_size=50, epochs=10, print_every=1):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    enc_optim = optim.SGD(encoder.parameters(), lr=0.001, momentum=0.99)\n",
    "    dec_optim = optim.SGD(decoder.parameters(), lr=0.001, momentum=0.99)\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        encoder.zero_grad(); enc_optim.zero_grad()\n",
    "        decoder.zero_grad(); dec_optim.zero_grad()\n",
    "\n",
    "        train_x, train_y = shuffle(train_x, train_y)\n",
    "\n",
    "        epoch_loss = 0\n",
    "        batch_loss = 0    \n",
    "        \n",
    "        for i in range(len(train_x)):\n",
    "            x = train_x[i]\n",
    "            y = train_y[i]\n",
    "            \n",
    "            batch_loss += decoder(encoder(x), y, START_IX, STOP_IX)\n",
    "\n",
    "            if (i+1) % batch_size == 0:\n",
    "                batch_loss.backward()\n",
    "                enc_optim.step()\n",
    "                dec_optim.step()\n",
    "\n",
    "                encoder.zero_grad(); enc_optim.zero_grad()\n",
    "                decoder.zero_grad(); dec_optim.zero_grad()\n",
    "\n",
    "                epoch_loss += batch_loss.item()\n",
    "                batch_loss = 0\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print(f\"**** Epoch {epoch} - Loss: {epoch_loss / len(train_x):.6f} ****\")\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (emb): Embedding(27, 16)\n",
      "  (lstm): LSTM(16, 32, batch_first=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Decoder(\n",
      "  (emb): Embedding(29, 16)\n",
      "  (lstm): LSTMCell(16, 32)\n",
      "  (clf): Linear(in_features=32, out_features=29, bias=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (objective): CrossEntropyLoss()\n",
      ")\n",
      "**** Epoch 1 - Loss: 3.361181 ****\n",
      "**** Epoch 2 - Loss: 3.208918 ****\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size=len(src_vocab), emb_dim=16, lstm_size=32, z_type=1) # 64 128\n",
    "decoder = Decoder(vocab_size=len(tgt_vocab), emb_dim=16, lstm_size=32) # 64 128\n",
    "\n",
    "print(encoder)\n",
    "print(decoder)\n",
    "\n",
    "encoder, decoder = train(encoder, decoder, train_x, train_y, batch_size=5, epochs=2, print_every=1) #50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
