# Assignment

As you can see, the models are quite good at the task at hand. However, they are not perfect and you will see that the performance on the test data drops. One of the reasons is that the models were trained for a few epochs. Even though training for longer could improve the models, the code is not efficient because it does not use batches. Instead, we are iterating sample by sample without taking advantage of parallelization. Once you optimize this code, you will see that both models are good, and the attention brings a substantial boost in performance.

### Your task

Your task is to make this model efficient using batches. You can keep the same overall architecture, which is a single LSTM layer with one direction on both the encoder and decoder while also using attention. **As requirement**, you must use RNN-based models with attention, but the number of layers, directions, and hidden units of the RNN are up to you. Similarly, the attention method (either any version of Luong's attention or Bahdanau's attention) can be defined by you. You can also use self-attention if you prefer, but that is not required nor needed to achieve good results.

More specifically, your notebook must include the following (gradable) aspects:
- the notebook must show all the epoch iterations with their respective losses.
- the resulting model must be selected based on the best performance on the validation set.
- the notebook must display the training and validation losses in a single plot (x-axis being the epochs, and y-axis being the cross-entropy loss).
- the notebook must display the results on the test set of your best model **according to validation set**.
- the notebook must include a **detailed** description of how you are implementing batches, not only the code.
- the notebook must specify in detail the architecture of your model.
- the notebook must include an analysis section at the end where you detail a comparison of two or more different models that at least differ from using vrs. not using attention mechanism.
- the notebook must be runnable by us, and it should skip training (**hint**: check if the `model.pt` file exists to skip training). We won't run the analysis section since that would require your other models to be included in the submission.


### Evaluation metric

We will measure the performance of your model using accuracy for the exact matches. Since this is a toy dataset, it's reasonable to measure the performance using this metric (it is worth mentioning that for tasks such as machine trasnlation, the official metric is BLEU). We will provide a test dataset (although auto-generated too) that is fixed for everyone to make all the systems comparable.

### Grading criteria

- 20% -> Encoder-decoder architecture.
- 30% -> Batch implementation.
- 20% -> Training loop and losses.
- 20% -> Analysis.
- 10% -> Performance.

Note that both **the code and the description** that you provide will be taken into account for every gradable aspect.

If you have any questions regarding the assignment, please post it on Piazza, contact us by email or meet us online during the office hours.

### Delivery

- The encoder and decoder models (a single binary file namedÂ model.pt)
- A runnable notebook with all your code (.ipynb file)
- An HTML version of your notebook (you can convert your .ipynb to .html from the notebook menu)

The deadline is **April 10, 2:00 PM**.

### A few words on the solution

A possible solution of the assignment uses the exact same architectures (same LSTM and attention mechanisms and parameters), but it was trained on batches. An iteration over the entire training set (a.k.a. epoch) took around 12 seconds with 50 samples per batch, which is 400 backpropagation steps from the 20k training samples. We trained the model for 100 epochs (about 20 minutes on a GPU) and reached over 90% of accuracy on the test set (90.06% for seq2seq and 97.04% for seq2seq+attention).

### The test data


**NOTE:** You can download the test data from [here](https://github.com/gaguilar/basic_nlp_tutorial/blob/master/tutorial_on_seq2seq_models/test.txt)

The test data is generated in the same manner as the training/validation data, but with intentionally longer input sentences. Here's how the current models perform on the test set:
