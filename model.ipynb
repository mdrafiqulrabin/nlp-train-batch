{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Md Rafiqul Islam Rabin\n",
    "### Email: mrabin@central.uh.edu\n",
    "### PS ID: 1797648\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I have adapted *Gustavo*'s [demo code](https://colab.research.google.com/github/gaguilar/basic_nlp_tutorial/blob/master/tutorial_on_seq2seq_models/Sequence%20to%20Sequence%20Models%20%28COSC%206336%29.ipynb) of Sequence to Sequence Models to implement the batch support for encoder and decoder. The task of the model is to map the sequences with repeated and unordered letters to alphabetically-sorted sequences of unique letters, i.e. *ccccaaabb -> abc*. The training and validation dataset are automatically-generated corpus, and the test dataset has been downloaded from [this link](https://github.com/gaguilar/basic_nlp_tutorial/blob/master/tutorial_on_seq2seq_models/test.txt) shared by *Gustavo*.\n",
    "\n",
    "The original demo code is not efficient because it does not use batches. Instead, it processes sample by sample without taking advantage of parallelization. The target of this assignment is to optimize the demo code using batch implementation. I will keep the same overall architecture of the demo code, which is a single LSTM layer with one direction on both the encoder and decoder while also using attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Importing packages:**\n",
    "\n",
    "I have imported all necessary packages first to run my code: **os** for checking if a path exists, **random** for getting a random number and shuffling the train set, **numpy** for getting transposed array and counting non-zeros in a list, **pandas** for reading the test CSV file, **torch** for PyTorch implementation of the model, **accuracy_score** for calculating accuracy, **_pickle** for dumping and loading loss values as a list, **datetime** for getting current time, **pyplot** for drawing plots, **copy** for copying an object in python, and **warnings** for filtering user warnings. We also set manual seed as 1 to **torch** and **random** for reproducing the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "import _pickle as pk\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "import copy, warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Config dictionary:**\n",
    "\n",
    "I have created a dictionary named **config** for configuration handling. I have defined the value of all parameters in this **config** dictionary to avoid the hard-coded value in code. The **num_train** denotes the number of training inputs and the **num_valid** denotes the number of validation inputs. The **pred_maxlen** means how many steps our model will predict given a test input if the end token is not found. Then, I have included the learning rate (**lr**), the momentum (**mt**), the embedding dimension (**emb_dim**), the hidden size of LSTM (**lstm_size**), the linear size of AttentionDecoder (**attn_size**), the dropout probability of model's regularization (**dropout**), the number of epochs (**epoch**), and the batch size (**batch**). Finally, I have also added the path of the test file (**testfile**), the checkpoint for the loss value (**lossfile**) and the encoder-decoder model (**checkpoint**). I will explain the usage of these parameters later while implementing the encoder-decoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'num_train': 20000,\n",
    "    'num_valid': 5000,\n",
    "    'pred_maxlen': 10,\n",
    "    \n",
    "    'lr': 0.001,\n",
    "    'mt': 0.99,\n",
    "    'emb_dim': 64,\n",
    "    'lstm_size': 128,\n",
    "    'attn_size': 100,\n",
    "    'dropout': 0.5,\n",
    "    'batch': 32,\n",
    "    'epoch': 50,\n",
    "    \n",
    "    'testfile': \"data/raw/test.txt\",\n",
    "    'lossfile': 'model.loss',\n",
    "    'checkpoint': \"model.pt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Creating the training and validation set:**\n",
    "\n",
    "Here, I have used *Gustavo*'s **sorting_letters_dataset** function that takes the **size** as an argument and returns the number of inputs. I have also created 20,000 samples for the training set and 5,000 samples for the validation set. Each sample has 3-10 letters that repeat 1-3 times. The training set is used to update the parameters of the model and the validation set is used to select the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_letters_dataset(size):\n",
    "    dataset = []\n",
    "    for _ in range(size):\n",
    "        x = []\n",
    "        for _ in range(random.randint(3, 10)):\n",
    "            letter = chr(random.randint(97, 122))\n",
    "            repeat = [letter] * random.randint(1, 3)\n",
    "            x.extend(repeat)\n",
    "        y = sorted(set(x))\n",
    "        dataset.append((x, y))\n",
    "    return zip(*dataset)\n",
    "\n",
    "train_inp, train_out = sorting_letters_dataset(config['num_train'])\n",
    "valid_inp, valid_out = sorting_letters_dataset(config['num_valid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Mapping dataset through vocabulary:**\n",
    "\n",
    "Here, I have used *Gustavo*'s **Vocab** class to handle the dataset. Our dataset contains letters (a-z) and we would like to map those letters to numbers. The source vocabulary (**src_vocab**) contains 27 tokens ($<$pad$>$ token and 26 letters) and the target vocabulary (**tgt_vocab**) contains 29 tokens ($<$pad$>$ token, 26 letters, $<$start$>$ and $<$stop$>$ token). The **map_elems** function maps a single input and **map_many_elems** function maps a list of inputs to numbers from letters. Using these helper functions, I have mapped the entire train and valid dataset. Additionally, I'm also storing the index of $<$pad$>$, $<$start$>$ and $<$stop$>$ token to be used in encoder/decoder and training/evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the training dataset: 20000\n",
      "Length of the validation dataset: 5000\n"
     ]
    }
   ],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, vocab):\n",
    "        self.itos = vocab\n",
    "        self.stoi = {d:i for i, d in enumerate(self.itos)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos) \n",
    "\n",
    "src_vocab = Vocab(['<pad>'] + [chr(i+97) for i in range(26)])\n",
    "tgt_vocab = Vocab(['<pad>'] + [chr(i+97) for i in range(26)] + ['<start>', '<stop>'] )\n",
    "\n",
    "def map_elems(elems, mapper):\n",
    "    return [mapper[elem] for elem in elems]\n",
    "\n",
    "def map_many_elems(many_elems, mapper):\n",
    "    return [map_elems(elems, mapper) for elems in many_elems]\n",
    "\n",
    "train_x = map_many_elems(train_inp, src_vocab.stoi)\n",
    "train_y = map_many_elems(train_out, tgt_vocab.stoi)\n",
    "print(\"Length of the training dataset: {}\".format(len(train_x)))\n",
    "\n",
    "valid_x = map_many_elems(valid_inp, src_vocab.stoi)\n",
    "valid_y = map_many_elems(valid_out, tgt_vocab.stoi)\n",
    "print(\"Length of the validation dataset: {}\".format(len(valid_x)))\n",
    "\n",
    "PAD_IX   = tgt_vocab.stoi['<pad>']\n",
    "START_IX = tgt_vocab.stoi['<start>']\n",
    "STOP_IX  = tgt_vocab.stoi['<stop>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Encoder:**\n",
    "\n",
    "**Architecture of the encoder**: The encoder is implemented with a simple LSTM model with one layer (**num_layers=1** by default) and one direction (**bidirectional=False** by default). Additionally, in the LSTM model, the number of expected features in the input is 64 (**emb_dim**), the number of features in the hidden state is 128 (**lstm_size**), and the input and output tensors are provided as **(batch, seq, feature)** as the **batch_first=True**. The encoder also includes one embedding layer and one dropout layer. In the embedding layer, the size of the dictionary of embeddings is 27 (**src_vocab**), and the size of each embedding vector is 64 (**emb_dim**). In the dropout layer, the probability of the activation of an element to be zeroed is 0.5 (**dropout**), it is only applied during training. This layer randomly sets some of its input to zero, which makes the final trained network more robust and less prone to overfitting.\n",
    "\n",
    "**Batch processing for encoder**:\n",
    "In order to support batch processing, I have used the [pad-pack mechanism](https://suzyahyah.github.io/pytorch/2019/07/01/DataLoader-Pad-Pack-Sequence.html). First, I have created a multi-dimensional tensor for containing the batch inputs. The batch inputs may have variable length tokens, but we need the fixed-length inputs for performing embeddings. Therefore, the **pad_sequence** function is used to convert the variable-length sequences to the same length using **padding_value=PAD_IX** for filling with zeros. Then, I have converted the padded sequences to embeddings and have applied the dropout regularization to prevent overfitting. However, the LSTM model wants only the meaningful timesteps of each input. For that, I have packed the padded-embedded sequence using **pack_padded_sequence** function, where we need to provide the length of each batch inputs. We also set **enforce_sorted=False** as we are dealing with unsorted sequences. Now, I have fed the packed data (that only keeps the meaningful timestep according to the lengths tensor) into the LSTM model and stored the output features, hidden state, and cell state from the last layer of the LSTM model. Finally, the **pad_packed_sequence** function is applied to the output features to convert into the padded output format. Note that I have used the **batch_first=True** in all scenarios as the tensor is provided as **(batch, seq, feature)**. Based on the **z_index** the encoder returns the output features as **(batch, seq, lstm_dim)** tensor or the (hidden state, cell state) as **(batch, lstm_dim)** tensor. The major modifications insides the **forward()** are: 1) padding the variable-length sequences to fixed-length sequences that are needed for applying embeddings, 2) packing the padded-embedded sequences as the LSTM model wants the meaningful timestep, and 3) padding the packed output features for making fixed-length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (emb): Embedding(27, 64)\n",
      "  (lstm): LSTM(64, 128, batch_first=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, lstm_size, z_type, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.z_index = z_type\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, lstm_size, batch_first=True)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, enc_inputs):\n",
    "        batch_inputs = copy.deepcopy(enc_inputs)\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        x_tensor = [torch.tensor(sample).to(device) for sample in batch_inputs]\n",
    "        x_pad = pad_sequence(x_tensor, batch_first=True, padding_value=PAD_IX) # (batch, seqlen) \n",
    "        x_emb = self.emb(x_pad) # (batch, seqlen, emb_dim) \n",
    "        x_emb = self.drop(x_emb)\n",
    "        \n",
    "        x_len = [len(sample) for sample in batch_inputs]\n",
    "        x_pack = pack_padded_sequence(x_emb, x_len, batch_first=True, enforce_sorted=False)\n",
    "        outs_pack, (h_n, c_n) = self.lstm(x_pack)\n",
    "        outs, _ = pad_packed_sequence(outs_pack, batch_first=True)\n",
    "            \n",
    "        if self.z_index == 1:\n",
    "            return h_n[0], c_n[0] # (seqlen, batch, lstm_dim)\n",
    "        else:\n",
    "            return outs # (batch, seqlen, lstm_dim)\n",
    "\n",
    "encoder = Encoder(vocab_size=len(src_vocab), \n",
    "                  emb_dim=config['emb_dim'], \n",
    "                  lstm_size=config['lstm_size'], \n",
    "                  z_type=0)\n",
    "\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Attention Decoder:**\n",
    "\n",
    "**Architecture of the decoder**: Similar to the encoder, the decoder is implemented with an LSTM cell with one layer and one direction. The attention version that we use for this implementation is Luong's attention, also known as multiplicative attention. The attention-decoder contains one embedding layer, one dropout layer, one LSTM cell, one attention layer, one linear layer. The LSTM cell in decoder has the same dimensions of the LSTM model in the encoder. However, in the embedding layer, the size of the dictionary of embeddings is 29 (**tgt_vocab**) as decoder needs two extra tokens: $<$start$>$ and $<$stop$>$. The attention class concatenates the encoder outputs with decoder states, multiplies by W that maps to the attention dimensions, squashes to the range [-1, 1] using **tanh**, and then multiplies by the scaler v that converts to single scores. After that, the scores are normalized with **softmax** and multiplied with encoder outputs to get the context from the weighted sum. The context is then concatenated with the decoder states and used for predicting the next step. The linear layer applies a linear transformation to the incoming combined data and computes the predicted scores. Finally, the **CrossEntropyLoss** is used to get the loss value that is sent for backpropagation. Also, during prediction, the argmax with softmax is applied to the predicted scores to compute the next token. During training, the teacher forcing mechanism has applied where we set the ground-truth token for calculating the next timestep instead of the previous predicted timestep. However, during evaluation and prediction, we should use the previously predicted timestep for calculating the next timestep.\n",
    "\n",
    "**Batch processing for decoder**: The **AttentionDecoder** class has to be modified in several areas in order to support batch processing. The major modifications insides the **forward()** function are: 1) Changing the **torch.zeros(1,)** to **torch.zeros(batch_size,)** inside the **init_state()**. 2) Padding the batch targets based on the max length inside the current batch (i.e., **pad_targets()**). 3) Creating the initial input tensor with **$<$start$>$ * batch_size** tokens instead of a single **$<$start$>$** token, as each timestep we expect the decoder to process the batch number of different sequence prediction in parallel and each will start from the **$<$start$>$** token. 4) Similarly, creating the target tensors based on the next token from each sample of batches. 5) Adding **ignore_index=PAD_IX** to the objective function to ignore the padding value in loss calculation, as we have added padding to make fixed-length the sequence prediction will continue till the maxlen. Therefore, the loss of padding value will be ignored. 6) Determining the mask length of each sample of batch inputs to calculate the batch loss. Similarly, during prediction I have made following further changes inside **predict()** function - 7) Adding the **dim=1** inside the argmax function for batch prediction, and transposing the prediction list to get prediction sequence for each input. 8) Removing the extra tokens after end token as the decoder continues till the maxlen for batch processing. I have also created another function called **evaluate()** inside the decoder same as the **predict()** function. The only difference is that the **predict()** does not requires ground-truth and returns only predicted sequence. On the other hand, the **evaluate()** requires ground-truth to compute the loss (i.e. validation loss) and returns both the loss and prediction of batch input. Note that during training and validation the max prediction size is determined by length of ground-truth targets inside each batch, but the prediction size is passed as an argument during prediction as we should not use the information of targets during prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionDecoder(\n",
      "  (emb): Embedding(29, 64)\n",
      "  (lstm): LSTMCell(64, 128)\n",
      "  (attn): Attention(\n",
      "    (W): Linear(in_features=256, out_features=100, bias=True)\n",
      "    (v): Linear(in_features=100, out_features=1, bias=False)\n",
      "  )\n",
      "  (clf): Linear(in_features=256, out_features=29, bias=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (objective): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, attn_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, attn_dim)\n",
    "        self.v = nn.Linear(attn_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, dec_hidden, enc_outs):\n",
    "        # enc_outs -> (batch, seqlen, hidden)\n",
    "        # dec_hidden -> (batch, hidden)\n",
    "        \n",
    "        seqlen = enc_outs.size(1)\n",
    "        \n",
    "        repeat_h = dec_hidden.unsqueeze(1)  # make room to repeat on seqlen dim\n",
    "        repeat_h = repeat_h.repeat(1, seqlen, 1)  # (1, seqlen, hidden)\n",
    "\n",
    "        concat_h = torch.cat((enc_outs, repeat_h), dim=2) # (1, seqlen, hidden*2)\n",
    "        \n",
    "        scores = self.v(torch.tanh(self.W(concat_h))) # (1, seqlen, 1)\n",
    "        probs = torch.softmax(scores, dim=1)\n",
    "        \n",
    "        weighted = enc_outs * probs # (1, seqlen, hidden)\n",
    "        \n",
    "        context = torch.sum(weighted, dim=1, keepdim=False) # (1, hidden)\n",
    "        combined = torch.cat((dec_hidden, context), dim=1)  # (1, hidden*2)\n",
    "        \n",
    "        return combined\n",
    "\n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, lstm_size, attn_size, dropout=0.5):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        \n",
    "        self.lstm_size = lstm_size\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTMCell(emb_dim, lstm_size)\n",
    "        self.attn = Attention(lstm_size * 2, attn_size)\n",
    "        self.clf = nn.Linear(lstm_size * 2, vocab_size)\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.objective = nn.CrossEntropyLoss(reduction=\"none\", ignore_index=PAD_IX)\n",
    "        \n",
    "    def init_state(self, batch_size, device):\n",
    "        h_0 = torch.zeros(batch_size, self.lstm_size).to(device)  # (batch, hidden_size)\n",
    "        c_0 = torch.zeros(batch_size, self.lstm_size).to(device)  # (batch, hidden_size)\n",
    "        return h_0, c_0\n",
    "    \n",
    "    def pad_targets(self, targets):\n",
    "        maxlen = max([len(target) for target in targets])\n",
    "        for i in range(len(targets)): \n",
    "            targets[i].append(STOP_IX) #added last token\n",
    "            targets[i].extend([PAD_IX] * (maxlen + 1 - len(targets[i]))) #added pad token\n",
    "        return targets, maxlen\n",
    "            \n",
    "    def forward(self, enc_outs, dec_targets, curr_token, last_token):\n",
    "        batch_targets = copy.deepcopy(dec_targets)\n",
    "        device = next(self.parameters()).device\n",
    "        batch_size = enc_outs.shape[0]\n",
    "        state = self.init_state(batch_size, device) # (batch, lstm_dim)\n",
    "        \n",
    "        batch_targets, maxlen = self.pad_targets(batch_targets)\n",
    "        \n",
    "        batch_loss = 0.0\n",
    "        curr_tokens = [curr_token] * batch_size\n",
    "        for i in range(maxlen + 1):\n",
    "            inputs = torch.tensor(curr_tokens).to(device) # (batch)\n",
    "            \n",
    "            emb = self.emb(inputs) # (batch, emb_dim)\n",
    "            emb = self.drop(emb) # (batch, emb_dim)\n",
    "            \n",
    "            state = self.lstm(emb, state) # (batch, lstm_dim)\n",
    "            q_i, _ = state \n",
    "            q_i = self.drop(q_i) # (batch, lstm_dim)\n",
    "            \n",
    "            combined = self.attn(q_i, enc_outs) # (batch, lstm_dim * 2)\n",
    "            scores = self.clf(combined) # (batch, tgt_vocab)\n",
    "            \n",
    "            next_tokens = [targets[i] for targets in batch_targets]\n",
    "            targets = torch.tensor(next_tokens).to(device) # (batch)\n",
    "            batch_loss += self.objective(scores, targets) # (batch)\n",
    "            \n",
    "            curr_tokens = next_tokens\n",
    "        \n",
    "        maskcount = [np.count_nonzero(targets) for targets in batch_targets]\n",
    "        maskcount = torch.tensor(maskcount, dtype=torch.float32).to(device)\n",
    "        batch_loss = (batch_loss/maskcount).sum()\n",
    "        \n",
    "        return batch_loss\n",
    "    \n",
    "    def predict(self, enc_outs, curr_token, last_token, maxlen):\n",
    "        device = next(self.parameters()).device\n",
    "        batch_size = enc_outs.shape[0]\n",
    "        state = self.init_state(batch_size, device) # (batch, lstm_dim)\n",
    "        \n",
    "        batch_preds = []\n",
    "        curr_tokens = [curr_token] * batch_size\n",
    "        for i in range(maxlen + 1):\n",
    "            inputs = torch.tensor(curr_tokens).to(device) # (batch)\n",
    "            \n",
    "            emb = self.emb(inputs) # (batch, emb_dim)\n",
    "            \n",
    "            state = self.lstm(emb, state) # (batch, lstm_dim)\n",
    "            q_i, _ = state \n",
    "            \n",
    "            combined = self.attn(q_i, enc_outs) # (batch, lstm_dim * 2)\n",
    "            scores = self.clf(combined) # (batch, tgt_vocab)\n",
    "            \n",
    "            pred_tokens = torch.argmax(torch.softmax(scores, dim=1), dim=1) # (batch)\n",
    "            curr_tokens = pred_tokens\n",
    "            batch_preds.append(pred_tokens.tolist())\n",
    "\n",
    "        batch_preds = np.array(batch_preds).T.tolist()\n",
    "        for ix, _ in enumerate(batch_preds):\n",
    "            if last_token in batch_preds[ix]:\n",
    "                last_token_ix = batch_preds[ix].index(last_token)\n",
    "                batch_preds[ix] = batch_preds[ix][:last_token_ix]\n",
    "        return batch_preds\n",
    "    \n",
    "    def evaluate(self, enc_outs, dec_targets, curr_token, last_token):\n",
    "        batch_targets = copy.deepcopy(dec_targets)\n",
    "        device = next(self.parameters()).device\n",
    "        batch_size = enc_outs.shape[0]\n",
    "        state = self.init_state(batch_size, device) # (batch, lstm_dim)\n",
    "        \n",
    "        batch_targets, maxlen = self.pad_targets(batch_targets)\n",
    "        \n",
    "        batch_preds, batch_loss = [], 0.0\n",
    "        curr_tokens = [curr_token] * batch_size\n",
    "        for i in range(maxlen + 1):\n",
    "            inputs = torch.tensor(curr_tokens).to(device) # (batch)\n",
    "            \n",
    "            emb = self.emb(inputs) # (batch, emb_dim)\n",
    "            \n",
    "            state = self.lstm(emb, state) # (batch, lstm_dim)\n",
    "            q_i, _ = state\n",
    "            \n",
    "            combined = self.attn(q_i, enc_outs) # (batch, lstm_dim * 2)\n",
    "            scores = self.clf(combined) # (batch, tgt_vocab)\n",
    "            \n",
    "            next_tokens = [targets[i] for targets in batch_targets]\n",
    "            targets = torch.tensor(next_tokens).to(device) # (batch)\n",
    "            batch_loss += self.objective(scores, targets) # (batch)\n",
    "            \n",
    "            pred_tokens = torch.argmax(torch.softmax(scores, dim=1), dim=1) # (batch)\n",
    "            curr_tokens = pred_tokens\n",
    "            batch_preds.append(pred_tokens.tolist())\n",
    "        \n",
    "        maskcount = [np.count_nonzero(targets) for targets in batch_targets]\n",
    "        maskcount = torch.tensor(maskcount, dtype=torch.float32).to(device)\n",
    "        batch_loss = (batch_loss/maskcount).sum()\n",
    "        \n",
    "        batch_preds = np.array(batch_preds).T.tolist()\n",
    "        for ix, _ in enumerate(batch_preds):\n",
    "            if last_token in batch_preds[ix]:\n",
    "                last_token_ix = batch_preds[ix].index(last_token)\n",
    "                batch_preds[ix] = batch_preds[ix][:last_token_ix]\n",
    "        \n",
    "        return batch_preds, batch_loss\n",
    "\n",
    "decoder = AttentionDecoder(vocab_size=len(tgt_vocab), \n",
    "                           emb_dim=config['emb_dim'], \n",
    "                           lstm_size=config['lstm_size'], \n",
    "                           attn_size=config['attn_size'])\n",
    "\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. Training loop:**\n",
    "\n",
    "I have adapted *Niloofar*'s [demo code](https://colab.research.google.com/drive/12a2m4axuWJOWGdynL925zGObcceskW1v#scrollTo=z51kxIBpfhCs) for Sentiment Analysis to build the training loop and to keep track the best model based on the accuracy of the validation set.\n",
    "\n",
    "The **training_loop()** function handles the training of the model. It first sets the encoder and decoder to the device (CPU or CUDA by checking if CUDA is available). Then, it initializes the SGD (Stochastic Gradient Descent) optimizer for encoder and decoder using the learning rate (=0.001) and the momentum (=0.99) from the config dictionary. For each epoch, it calls the **train()** function to train the model and saves the training loss. After training for an epoch, it calls the **evaluate()** function to get the validation loss and validation accuracy. Based on this validation accuracy, it keeps track of the best model by calling **track_best_model()** function that saves the best model to disk using **torch.save()**. The training will continue until it reaches the maximum epochs. I'm using 50 as the maximum number of epochs to be continued. The model will continue training for 50 epochs and will save the best model according to the validation accuracy. For each epoch, it also displays a log with the current time (see **getCurrentTime()**), epoch, training loss, validation loss, and validation accuracy. After finishing the full training, it dumps the training losses and validation losses to disk using **pickle.dump()**.\n",
    "\n",
    "The **train()** function first actives the training mode for encoder and decoder by calling **train()**. This is important as layers like dropout has different behavior during train and evaluation. Then, it shuffles the training data (by calling **shuffle()**) before every epoch to prevent any bias of the order of the training. For each batch, it clears the old gradients (by calling **zero_grad()**) that prevents the model to accumulate the gradients from all previous loss. Then, it invokes the **forward()** of the decoder with batch data and completes the forward pass that returns the batch loss. Then, it calls the **loss.backward()** to compute the derivative of the loss and the **optim.step()** to take a step towards the minimum of the loss function. Finally, it sends the training loss to the caller function.\n",
    "\n",
    "The **evaluate()** function first actives the evaluation mode for encoder and decoder by calling **eval()**. This is important as layers like dropout has different behavior during train and evaluation. Then, it deactivates the autograd (by calling **torch.no_grad()**) to speed up the evaluation and to disable the backpropagation. Then, it invokes the **evaluate()** of the decoder with batch data that returns the batch prediction and batch loss. By calling **map_prediction()**, it converts the numbers to letters using **tgt_vocab.itos** (i.e. [1,2,3] --> ['a','b','c'] --> 'abc']). Finaly, it computs the accurcay between the predictions and actuals, and sends the validation loss  and validation accuracy to the caller function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_prediction(sample_preds):\n",
    "    sample_preds = [[tgt_vocab.itos[ix] for ix in each_preds] for each_preds in sample_preds]\n",
    "    sample_preds = [''.join(each_preds) for each_preds in sample_preds]\n",
    "    return sample_preds\n",
    "\n",
    "def shuffle(x, y):\n",
    "    pack = list(zip(x, y))\n",
    "    random.shuffle(pack)\n",
    "    return zip(*pack)\n",
    "\n",
    "def evaluate(encoder, decoder, sample_x, sample_y, batch_size):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    sample_loss = 0.0\n",
    "    batch_x, batch_y = [], []\n",
    "    predictions, actuals = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(sample_x)):\n",
    "            batch_x.append(sample_x[i])\n",
    "            batch_y.append(sample_y[i])\n",
    "            \n",
    "            if len(batch_x) == batch_size or i == len(sample_x) - 1:\n",
    "                batch_preds, batch_loss = decoder.evaluate(encoder(batch_x), batch_y, START_IX, STOP_IX)\n",
    "                \n",
    "                batch_preds = map_prediction(batch_preds)\n",
    "                predictions.extend(batch_preds)\n",
    "                batch_y = map_prediction(batch_y)\n",
    "                actuals.extend(batch_y)\n",
    "                \n",
    "                sample_loss += batch_loss.item()\n",
    "                batch_x, batch_y = [], []\n",
    "    \n",
    "    sample_loss = sample_loss / len(sample_x) * 1.0\n",
    "    \n",
    "    accuracy = accuracy_score(actuals, predictions)\n",
    "    return predictions, sample_loss, accuracy\n",
    "\n",
    "def train(encoder, enc_optim, decoder, dec_optim, train_x, train_y, batch_size):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    train_x, train_y = shuffle(train_x, train_y)\n",
    "    batch_x, batch_y = [], []\n",
    "\n",
    "    for i in range(len(train_x)):\n",
    "        batch_x.append(train_x[i])\n",
    "        batch_y.append(train_y[i])\n",
    "\n",
    "        if len(batch_x) == batch_size or i == len(train_x) - 1:\n",
    "            encoder.zero_grad(); enc_optim.zero_grad()\n",
    "            decoder.zero_grad(); dec_optim.zero_grad()\n",
    "        \n",
    "            batch_loss = decoder(encoder(batch_x), batch_y, START_IX, STOP_IX)\n",
    "\n",
    "            batch_loss.backward()\n",
    "            enc_optim.step()\n",
    "            dec_optim.step()\n",
    "\n",
    "            train_loss += batch_loss.item()\n",
    "            batch_x, batch_y = [], []\n",
    "\n",
    "    train_loss = train_loss / len(train_x) * 1.0\n",
    "    \n",
    "    return encoder, decoder, train_x, train_y, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCurrentTime():\n",
    "    return str(datetime.datetime.now())\n",
    "\n",
    "def track_best_model(encoder, decoder, epoch, best_acc, valid_acc, valid_loss):\n",
    "    if best_acc >= valid_acc:\n",
    "        return best_acc, ''\n",
    "    state = {\n",
    "        'encoder': encoder.state_dict(), \n",
    "        'decoder': decoder.state_dict(),\n",
    "        'acc': valid_acc,\n",
    "        'loss': valid_loss,\n",
    "        'epoch': epoch\n",
    "    }\n",
    "    torch.save(state, config['checkpoint'])\n",
    "    return valid_acc, ' * '\n",
    "\n",
    "def training_loop(encoder, decoder, train_x, train_y, epochs, batch_size, print_every=1):\n",
    "    print(\"\\nTraining with encoder and decoder...\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"\\nAttaching to device: {}\".format(device))\n",
    "    \n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    enc_optim = optim.SGD(encoder.parameters(), lr=config['lr'], momentum=config['mt'])\n",
    "    dec_optim = optim.SGD(decoder.parameters(), lr=config['lr'], momentum=config['mt'])\n",
    "    \n",
    "    best_acc = -1.0\n",
    "    keep_loss = [[], []] # [[train],[valid]]\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        encoder, decoder, train_x, train_y, train_loss = train(encoder, enc_optim, decoder, dec_optim, train_x, train_y, batch_size)\n",
    "        _, valid_loss, valid_acc = evaluate(encoder, decoder, valid_x, valid_y, batch_size)\n",
    "        best_acc, epoch_track = track_best_model(encoder, decoder, epoch, best_acc, valid_acc, valid_loss)\n",
    "\n",
    "        \n",
    "        keep_loss[0].append(train_loss)\n",
    "        keep_loss[1].append(valid_loss)\n",
    "        \n",
    "        epoch_msg = '\\n[{}] Epoch {:02d}: [TRAIN] Loss: {:.6f}'.format(getCurrentTime(), epoch, train_loss)\n",
    "        epoch_msg += ' [VAL] Loss: {:.6f}, Acc: {:.6f}'.format(valid_loss, valid_acc)\n",
    "        print(epoch_msg + epoch_track)\n",
    "    \n",
    "    with open(config['lossfile'], 'wb') as lossfile:\n",
    "        pk.dump(keep_loss, lossfile)\n",
    "    \n",
    "    print(\"\\nTraining completed...\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with encoder and decoder...\n",
      "\n",
      "Attaching to device: cuda\n",
      "\n",
      "[2020-04-11 16:26:35.167664] Epoch 01: [TRAIN] Loss: 1.326269 [VAL] Loss: 1.308529, Acc: 0.734600 * \n",
      "\n",
      "[2020-04-11 16:26:49.735896] Epoch 02: [TRAIN] Loss: 0.277904 [VAL] Loss: 0.377637, Acc: 0.935000 * \n",
      "\n",
      "[2020-04-11 16:27:04.244988] Epoch 03: [TRAIN] Loss: 0.154015 [VAL] Loss: 0.202143, Acc: 0.969000 * \n",
      "\n",
      "[2020-04-11 16:27:18.818182] Epoch 04: [TRAIN] Loss: 0.107612 [VAL] Loss: 0.149917, Acc: 0.976800 * \n",
      "\n",
      "[2020-04-11 16:27:33.322391] Epoch 05: [TRAIN] Loss: 0.086642 [VAL] Loss: 0.083299, Acc: 0.987800 * \n",
      "\n",
      "[2020-04-11 16:27:47.822166] Epoch 06: [TRAIN] Loss: 0.082515 [VAL] Loss: 0.109729, Acc: 0.983000\n",
      "\n",
      "[2020-04-11 16:28:02.401725] Epoch 07: [TRAIN] Loss: 0.086080 [VAL] Loss: 0.140087, Acc: 0.978600\n",
      "\n",
      "[2020-04-11 16:28:16.896313] Epoch 08: [TRAIN] Loss: 0.074891 [VAL] Loss: 0.065171, Acc: 0.991400 * \n",
      "\n",
      "[2020-04-11 16:28:31.468322] Epoch 09: [TRAIN] Loss: 0.063610 [VAL] Loss: 0.054420, Acc: 0.992800 * \n",
      "\n",
      "[2020-04-11 16:28:45.977070] Epoch 10: [TRAIN] Loss: 0.060919 [VAL] Loss: 0.044879, Acc: 0.994400 * \n",
      "\n",
      "[2020-04-11 16:29:00.478472] Epoch 11: [TRAIN] Loss: 0.055709 [VAL] Loss: 0.069564, Acc: 0.992600\n",
      "\n",
      "[2020-04-11 16:29:15.055772] Epoch 12: [TRAIN] Loss: 0.053759 [VAL] Loss: 0.044022, Acc: 0.993600\n",
      "\n",
      "[2020-04-11 16:29:29.546109] Epoch 13: [TRAIN] Loss: 0.052262 [VAL] Loss: 0.055785, Acc: 0.992000\n",
      "\n",
      "[2020-04-11 16:29:44.102149] Epoch 14: [TRAIN] Loss: 0.046426 [VAL] Loss: 0.038919, Acc: 0.996600 * \n",
      "\n",
      "[2020-04-11 16:29:58.608590] Epoch 15: [TRAIN] Loss: 0.045396 [VAL] Loss: 0.022515, Acc: 0.996400\n",
      "\n",
      "[2020-04-11 16:30:13.204810] Epoch 16: [TRAIN] Loss: 0.044725 [VAL] Loss: 0.049286, Acc: 0.993800\n",
      "\n",
      "[2020-04-11 16:30:27.723505] Epoch 17: [TRAIN] Loss: 0.043180 [VAL] Loss: 0.047444, Acc: 0.995400\n",
      "\n",
      "[2020-04-11 16:30:42.235462] Epoch 18: [TRAIN] Loss: 0.043407 [VAL] Loss: 0.022144, Acc: 0.997200 * \n",
      "\n",
      "[2020-04-11 16:30:56.816526] Epoch 19: [TRAIN] Loss: 0.038683 [VAL] Loss: 0.032425, Acc: 0.996400\n",
      "\n",
      "[2020-04-11 16:31:11.346106] Epoch 20: [TRAIN] Loss: 0.041567 [VAL] Loss: 0.030613, Acc: 0.996000\n",
      "\n",
      "[2020-04-11 16:31:25.905513] Epoch 21: [TRAIN] Loss: 0.042833 [VAL] Loss: 0.040503, Acc: 0.996000\n",
      "\n",
      "[2020-04-11 16:31:40.309486] Epoch 22: [TRAIN] Loss: 0.039456 [VAL] Loss: 0.030600, Acc: 0.996600\n",
      "\n",
      "[2020-04-11 16:31:54.710544] Epoch 23: [TRAIN] Loss: 0.040326 [VAL] Loss: 0.032159, Acc: 0.995000\n",
      "\n",
      "[2020-04-11 16:32:09.221633] Epoch 24: [TRAIN] Loss: 0.043830 [VAL] Loss: 0.028464, Acc: 0.994600\n",
      "\n",
      "[2020-04-11 16:32:23.653690] Epoch 25: [TRAIN] Loss: 0.042994 [VAL] Loss: 0.052393, Acc: 0.994800\n",
      "\n",
      "[2020-04-11 16:32:38.122178] Epoch 26: [TRAIN] Loss: 0.042672 [VAL] Loss: 0.056151, Acc: 0.993400\n",
      "\n",
      "[2020-04-11 16:32:52.528638] Epoch 27: [TRAIN] Loss: 0.039462 [VAL] Loss: 0.029573, Acc: 0.996600\n",
      "\n",
      "[2020-04-11 16:33:06.964749] Epoch 28: [TRAIN] Loss: 0.040575 [VAL] Loss: 0.014874, Acc: 0.998400 * \n",
      "\n",
      "[2020-04-11 16:33:21.452145] Epoch 29: [TRAIN] Loss: 0.041296 [VAL] Loss: 0.029702, Acc: 0.996400\n",
      "\n",
      "[2020-04-11 16:33:35.876851] Epoch 30: [TRAIN] Loss: 0.038535 [VAL] Loss: 0.017465, Acc: 0.997800\n",
      "\n",
      "[2020-04-11 16:33:50.372357] Epoch 31: [TRAIN] Loss: 0.036948 [VAL] Loss: 0.011558, Acc: 0.999000 * \n",
      "\n",
      "[2020-04-11 16:34:04.792159] Epoch 32: [TRAIN] Loss: 0.039509 [VAL] Loss: 0.058063, Acc: 0.995400\n",
      "\n",
      "[2020-04-11 16:34:19.215176] Epoch 33: [TRAIN] Loss: 0.041468 [VAL] Loss: 0.018154, Acc: 0.998000\n",
      "\n",
      "[2020-04-11 16:34:33.702649] Epoch 34: [TRAIN] Loss: 0.047692 [VAL] Loss: 0.007775, Acc: 0.999000\n",
      "\n",
      "[2020-04-11 16:34:48.129144] Epoch 35: [TRAIN] Loss: 0.043887 [VAL] Loss: 0.030024, Acc: 0.996400\n",
      "\n",
      "[2020-04-11 16:35:02.614443] Epoch 36: [TRAIN] Loss: 0.042824 [VAL] Loss: 0.025703, Acc: 0.995600\n",
      "\n",
      "[2020-04-11 16:35:17.005056] Epoch 37: [TRAIN] Loss: 0.045349 [VAL] Loss: 0.020100, Acc: 0.998200\n",
      "\n",
      "[2020-04-11 16:35:31.438151] Epoch 38: [TRAIN] Loss: 0.045571 [VAL] Loss: 0.015790, Acc: 0.997800\n",
      "\n",
      "[2020-04-11 16:35:45.927498] Epoch 39: [TRAIN] Loss: 0.044526 [VAL] Loss: 0.022560, Acc: 0.998000\n",
      "\n",
      "[2020-04-11 16:36:00.354432] Epoch 40: [TRAIN] Loss: 0.045641 [VAL] Loss: 0.021431, Acc: 0.997600\n",
      "\n",
      "[2020-04-11 16:36:14.841999] Epoch 41: [TRAIN] Loss: 0.051122 [VAL] Loss: 0.018581, Acc: 0.998400\n",
      "\n",
      "[2020-04-11 16:36:29.260982] Epoch 42: [TRAIN] Loss: 0.048637 [VAL] Loss: 0.040520, Acc: 0.995800\n",
      "\n",
      "[2020-04-11 16:36:43.676177] Epoch 43: [TRAIN] Loss: 0.042668 [VAL] Loss: 0.017035, Acc: 0.998400\n",
      "\n",
      "[2020-04-11 16:36:58.157490] Epoch 44: [TRAIN] Loss: 0.041984 [VAL] Loss: 0.014107, Acc: 0.998800\n",
      "\n",
      "[2020-04-11 16:37:12.572507] Epoch 45: [TRAIN] Loss: 0.047140 [VAL] Loss: 0.018522, Acc: 0.998000\n",
      "\n",
      "[2020-04-11 16:37:27.049790] Epoch 46: [TRAIN] Loss: 0.047383 [VAL] Loss: 0.010966, Acc: 0.999200 * \n",
      "\n",
      "[2020-04-11 16:37:41.459995] Epoch 47: [TRAIN] Loss: 0.041236 [VAL] Loss: 0.031945, Acc: 0.997000\n",
      "\n",
      "[2020-04-11 16:37:55.978579] Epoch 48: [TRAIN] Loss: 0.038749 [VAL] Loss: 0.006929, Acc: 0.999000\n",
      "\n",
      "[2020-04-11 16:38:10.399940] Epoch 49: [TRAIN] Loss: 0.042980 [VAL] Loss: 0.007858, Acc: 0.999200\n",
      "\n",
      "[2020-04-11 16:38:24.843154] Epoch 50: [TRAIN] Loss: 0.041912 [VAL] Loss: 0.021220, Acc: 0.997400\n",
      "\n",
      "Training completed...\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(config['checkpoint']):\n",
    "    training_loop(encoder, decoder, train_x, train_y, config['epoch'], config['batch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **8. Loading the best model:**\n",
    "\n",
    "During training, the best model has been saved to disk using **torch.save()** at the checkpoint. If the checkpoint is available, this code will skip the training loop. Otherwise, it will continue the training and will save the best model to disk at the checkpoint. The **load_best_model()** function loads the encoder and decoder from the checkpoint using **torch.load()**. It first initializes the encoder and decoder objects and loads the model's parameter using **load_state_dict**. The training losses and validation losses are also saved using **pickle.dump()** to disk after training and have been loaded from disk using **pickle.load()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving best model from epoch 46 with [DEV] loss 0.010966 and accuracy 0.999200.\n"
     ]
    }
   ],
   "source": [
    "def load_best_model():\n",
    "    encoder = Encoder(vocab_size=len(src_vocab), \n",
    "                  emb_dim=config['emb_dim'], \n",
    "                  lstm_size=config['lstm_size'], \n",
    "                  z_type=0)\n",
    "    decoder = AttentionDecoder(vocab_size=len(tgt_vocab), \n",
    "                               emb_dim=config['emb_dim'], \n",
    "                               lstm_size=config['lstm_size'], \n",
    "                               attn_size=config['attn_size'])\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    state = torch.load(config['checkpoint'], map_location=device)\n",
    "    encoder.load_state_dict(state['encoder'])\n",
    "    decoder.load_state_dict(state['decoder'])\n",
    "    state = {'acc': state['acc'], 'loss': state['loss'], 'epoch': state['epoch']}\n",
    "    return encoder, decoder, state\n",
    "\n",
    "keep_loss = [[], []] # [[train],[valid]]\n",
    "with open(config['lossfile'], 'rb') as lossfile:\n",
    "    keep_loss = pk.load(lossfile)\n",
    "encoder, decoder, state = load_best_model()\n",
    "print('Retrieving best model from epoch {} with [DEV] loss {:.6f} and accuracy {:.6f}.'.format(state['epoch'], state['loss'], state['acc'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **9. Learning curve for training and validation losses:**\n",
    "\n",
    "Here, I'm displaying the training and validation losses in a single plot (x-axis is the epochs, and the y-axis is the cross-entropy loss). The blue line represents the training losses and the green line represents the validation losses. Initially, both the training and validation losses are very high (up to 4 epochs) and then it decreases significantly over epochs. However, validation losses are not always lower than training losses. The plot (also see the log of training loop) shows that the validation loss is higher than training loss in most cases up to 7 epochs. Then the training and validation losses decrease with a minimal gap between them up to 32 epochs. After that, the validation losses become consistently smaller than the training losses up to 50 epochs. The plot also demonstrates that the validation losses decrease to a point of stability at the end and maintains a small gap with the training losses. Therefore, the learning curve shows a good fit. Note that I'm attaching an additional file ('model.loss') in my submission to plot this learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = [i for i in range(1, len(keep_loss[0])+1)]\n",
    "plt.plot(epochs, keep_loss[0], 'b', label=\"Training loss\")\n",
    "plt.plot(epochs, keep_loss[1], 'g', label=\"Validation loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.xticks(np.arange(0,config['epoch']+1,5))\n",
    "plt.ylabel('Cross-entropy loss')\n",
    "plt.title('The training and validation losses.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **10. Loading the test set:**\n",
    "\n",
    "Here, I have read the test dataset and mapped the test dataset to numbers from letters using vocabulary helper functions mentioned in section 4. The test dataset contains 5,000 test inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the test dataset: 5000\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(config['testfile'], delimiter='\\t', header=None, usecols=[0,1])\n",
    "test_inp, test_out = test_data[0], test_data[1]  \n",
    "\n",
    "test_x = map_many_elems(test_inp, src_vocab.stoi)\n",
    "test_y = map_many_elems(test_out, tgt_vocab.stoi)\n",
    "\n",
    "print(\"Length of the test dataset: {}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **11. Result on the test set:**\n",
    "\n",
    "Here, I'm displaying the result on the test set of my best model according to the accuracy of the validation set. I have measured the performance of my model using accuracy for the exact matches, i.e. if the truth label is 'abcd' and model predicts 'abcdd', then still it is a wrong prediction. The best model has been found at epoch 46 with the validation accuracy of **99.92%**. The accuracy on the test set is **97.74%** for pred_maxlen=10 and **99.60%** for pred_maxlen=20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set (max prediction size = 10) = 97.74%\n",
      "Accuracy on the test set (max prediction size = 20) = 99.60%\n"
     ]
    }
   ],
   "source": [
    "def predict(encoder, decoder, sample_x, batch_size, pred_size):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    batch_x = []\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(sample_x)):\n",
    "            batch_x.append(sample_x[i])\n",
    "            \n",
    "            if len(batch_x) == batch_size or i == len(sample_x) - 1:\n",
    "                batch_preds = decoder.predict(encoder(batch_x), START_IX, STOP_IX, pred_size)\n",
    "                batch_preds = map_prediction(batch_preds)\n",
    "                predictions.extend(batch_preds)\n",
    "                batch_x = []\n",
    "            \n",
    "    return predictions\n",
    "\n",
    "def getAccuracyScore(encoder, decoder, sample_x, sample_out, batch_size, pred_size):\n",
    "    predictions = predict(encoder, decoder, sample_x, batch_size, pred_size)\n",
    "    groundtruth = [''.join(str_y) for str_y in sample_out]\n",
    "    acc = accuracy_score(groundtruth, predictions)\n",
    "    return acc\n",
    "\n",
    "config['pred_maxlen'] = 10\n",
    "test_acc = getAccuracyScore(encoder, decoder, test_x, test_out, config['batch'], config['pred_maxlen']) * 100\n",
    "print('Accuracy on the test set (max prediction size = {}) = {:.2f}%'.format(config['pred_maxlen'], test_acc))\n",
    "\n",
    "config['pred_maxlen'] = 20\n",
    "test_acc = getAccuracyScore(encoder, decoder, test_x, test_out, config['batch'], config['pred_maxlen']) * 100\n",
    "print('Accuracy on the test set (max prediction size = {}) = {:.2f}%'.format(config['pred_maxlen'], test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **12. Analysis of results:**\n",
    "\n",
    "In this section, I will detail comparison of different models according to the following aspects:\n",
    "<ol>\n",
    "  <li> Using different epoch (5 vs 50). </li>\n",
    "  <li> Using batch implementation vs Not using batch implementation. </li>\n",
    "  <li> Using attention mechanism vs Not using attention mechanism. </li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "| Batch?| Attention?| Epoch | Val Accuracy | Test Accuracy |\n",
    "| ---   | ---       | ---   | ---          | ---           |\n",
    "| No    | No        | 5     | 98.20        | 86.70         |\n",
    "| No    | Yes       | 5     | 98.66        | 90.16         |\n",
    "| No    | No        | 50    | 99.78        | 91.84         |\n",
    "| No    | Yes       | 50    | 99.96        | 92.54         |\n",
    "| Yes   | No        | 5     | 97.58        | 90.24         |\n",
    "| Yes   | Yes       | 5     | 98.78        | 95.26         |\n",
    "| Yes   | No        | 50    | 99.98        | 97.80         |\n",
    "| Yes   | Yes       | 50    | 99.92        | 97.74         |\n",
    "\n",
    "The above table shows the result for the batch implementation (batch=32), attention mechanism (attn_size=100) and the number of epochs (epoch=50). Note that I have conducted this analysis using max prediction size = 10 for the test dataset (same as the demo code). Later, I will show another observation with different prediction sizes.\n",
    "\n",
    "The observations are -\n",
    "<ol>\n",
    "  <li> Epoch: Running the model with 50 epochs increases the accuracy on the validation set and test set in all scenarios compared to the 5 epochs. If the model does not use the attention, the accuracy in the test set improves by 5% for 50 epochs. If the model uses attention, the accuracy in the test set improves by 2% for 50 epochs. On the other hand, the accuracy in the validation set improves by 1% approximately for 50 epochs.\n",
    "  <li> Batch: Running the model with batch implementation increases the accuracy on the test approximately by 5% in all scenarios. However, there is no unique pattern in the validation accuracy and the change is below 1% as well.\n",
    "  <li> Attention: Running the model with attention mechanism increases the accuracy on the validation set and test set in most of the scenarios except for batch implementation with 50 epochs. Using batch implementation and 50 epochs, the model without attention mechanism converges to the better performance than the model with attention mechanism, however, the change is only 0.06% for both the validation and test dataset.\n",
    "</ol>\n",
    "\n",
    "**More complex dataset:**\n",
    "\n",
    "To study the attention mechanism, I have run an additional experiment (epoch=50, batch=32, pred_maxlen=10) with more complex and different training and validation set, and have compared the result with the corresponding non-attention mechanism. To generate a more complex and different dataset, I have increased the range of repeated letters from 1-3 to 5-10 and also changed the seed value from 1 to 42. It impacts both the training and validation dataset but keeps the number of total examples the same as before. The results are given below -\n",
    "\n",
    "| Attention? | Seed  | Range | Val Accuracy | Test Accuracy |\n",
    "| ---        | ---   | ---   | ---          | ---           |\n",
    "| No         | 42    |  1-3  | 99.94        | 27.46         |\n",
    "| Yes        | 42    |  5-10 | 99.76        | 76.96         |\n",
    "\n",
    "Based on the result of the above table, using a more complex and different dataset, we can clearly see that the attention mechanism outperforms the non-attention mechanism by 49.5% on the test dataset. This indicates that the non-attention mechanism may perform better under certain conditions but the attention mechanism performs significantly better in more complex scenarios. \n",
    "\n",
    "**Different prediction sizes:**\n",
    "\n",
    "It has been said by *Gustavo* that the test dataset is generated in the same manner as the training/validation data, but with intentionally longer input sentences. Therefore, I have conducted an additional analysis (epoch=50, batch=32) with different prediction sizes for the test dataset. Note that the prediction size has no effect on the train and validation dataset as the prediction size is determined by targets inside the batch. The results are given below -\n",
    "\n",
    "|Attention?| Prediction Size | Test Accuracy |\n",
    "| ---      | ---   | ---    |\n",
    "| No       | 10    | 97.80  |\n",
    "| No       | 20    | 99.70  |\n",
    "| Yes      | 10    | 97.74  |\n",
    "| Yes      | 20    | 99.60  |\n",
    "\n",
    "\n",
    "The result of the above table shows that the accuracy on the test dataset increased by 2% using longer prediction size (20 instead of 10). Note that I'm attaching an additional folder ('Analysis') in my submission with different experimental files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "\n",
    "<ol>\n",
    "    <li> https://pytorch.org/docs/stable/nn.html </li>\n",
    "    <li> https://colab.research.google.com/github/gaguilar/basic_nlp_tutorial/blob/master/tutorial_on_seq2seq_models/Sequence%20to%20Sequence%20Models%20(COSC%206336).ipynb </li>\n",
    "    <li> https://colab.research.google.com/drive/12a2m4axuWJOWGdynL925zGObcceskW1v </li>\n",
    "    <li> https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html </li>\n",
    "    <li> https://matplotlib.org/tutorials/introductory/pyplot.html </li>\n",
    "    <li> https://suzyahyah.github.io/pytorch/2019/07/01/DataLoader-Pad-Pack-Sequence.html </li>\n",
    "    <li> https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
