{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import _pickle as pk\n",
    "\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "import copy, warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing imports and config... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'num_train': 20000,\n",
    "    'num_valid': 5000,\n",
    "    'patience': 10,\n",
    "    'batch': 32,\n",
    "    'epoch': 1000,\n",
    "    'lr': 0.001,\n",
    "    'momentum': 0.99,\n",
    "    'emb_size': 64,\n",
    "    'lstm_size': 128,\n",
    "    'attn_size': 100,\n",
    "    'pred_size': 10,\n",
    "    'testfile': \"../data/raw/test.txt\",\n",
    "    'logfile': \"model_single_withattn.log\",\n",
    "    'lossfile': 'model_single_withattn.loss',\n",
    "    'checkpoint': \"model_single_withattn.pt\"\n",
    "}\n",
    "\n",
    "open(config['logfile'], 'w').close()\n",
    "def saveLogMsg(msg):\n",
    "    print(msg, \"\\n\")\n",
    "    with open(config['logfile'], \"a\") as myfile:\n",
    "        myfile.write(msg + \"\\n\")\n",
    "\n",
    "saveLogMsg(\"Initializing imports and config...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for train and valid... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sorting_letters_dataset(size):\n",
    "    dataset = []\n",
    "    for _ in range(size):\n",
    "        x = []\n",
    "        for _ in range(random.randint(3, 10)):\n",
    "            letter = chr(random.randint(97, 122))\n",
    "            repeat = [letter] * random.randint(1, 3)\n",
    "            x.extend(repeat)\n",
    "        y = sorted(set(x))\n",
    "        dataset.append((x, y))\n",
    "    return zip(*dataset)\n",
    "\n",
    "train_inp, train_out = sorting_letters_dataset(config['num_train'])\n",
    "valid_inp, valid_out = sorting_letters_dataset(config['num_valid'])\n",
    "\n",
    "saveLogMsg(\"Dataset for train and valid...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab for source and target... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, vocab):\n",
    "        self.itos = vocab\n",
    "        self.stoi = {d:i for i, d in enumerate(self.itos)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos) \n",
    "\n",
    "src_vocab = Vocab(['<pad>'] + [chr(i+97) for i in range(26)])\n",
    "tgt_vocab = Vocab(['<pad>'] + [chr(i+97) for i in range(26)] + ['<start>', '<stop>'] )\n",
    "\n",
    "START_IX = tgt_vocab.stoi['<start>']\n",
    "STOP_IX  = tgt_vocab.stoi['<stop>']\n",
    "\n",
    "saveLogMsg(\"Vocab for source and target...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping dataset through Vocab... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def map_elems(elems, mapper):\n",
    "    return [mapper[elem] for elem in elems]\n",
    "\n",
    "def map_many_elems(many_elems, mapper):\n",
    "    return [map_elems(elems, mapper) for elems in many_elems]\n",
    "\n",
    "train_x = map_many_elems(train_inp, src_vocab.stoi)\n",
    "train_y = map_many_elems(train_out, tgt_vocab.stoi)\n",
    "\n",
    "valid_x = map_many_elems(valid_inp, src_vocab.stoi)\n",
    "valid_y = map_many_elems(valid_out, tgt_vocab.stoi)\n",
    "\n",
    "saveLogMsg(\"Mapping dataset through Vocab...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder:\n",
      "Encoder(\n",
      "  (emb): Embedding(27, 64)\n",
      "  (lstm): LSTM(64, 128, batch_first=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ") \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, lstm_size, z_type, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.z_index = z_type\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, lstm_size, batch_first=True)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        seq = torch.tensor([inputs]).to(device) # (1, seqlen)\n",
    "        emb = self.emb(seq) # (1, seqlen, emb_dim)\n",
    "        emb = self.drop(emb) \n",
    "        \n",
    "        outs, (h_n, c_n) = self.lstm(emb)\n",
    "        \n",
    "        if self.z_index == 1:\n",
    "            return h_n[0], c_n[0] # (seqlen, lstm_dim)\n",
    "        else:\n",
    "            return outs # (1, seqlen, lstm_dim)\n",
    "\n",
    "encoder = Encoder(vocab_size=len(src_vocab), \n",
    "                  emb_dim=config['emb_size'], \n",
    "                  lstm_size=config['lstm_size'], \n",
    "                  z_type=0)\n",
    "saveLogMsg(\"encoder:\\n{}\".format(encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, attn_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, attn_dim)\n",
    "        self.v = nn.Linear(attn_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, dec_hidden, enc_outs):\n",
    "        # enc_outs -> (batch, seqlen, hidden)\n",
    "        # dec_hidden -> (batch, hidden)\n",
    "        \n",
    "        seqlen = enc_outs.size(1)\n",
    "        \n",
    "        repeat_h = dec_hidden.unsqueeze(1)  # make room to repeat on seqlen dim\n",
    "        repeat_h = repeat_h.repeat(1, seqlen, 1)  # (1, seqlen, hidden)\n",
    "\n",
    "        concat_h = torch.cat((enc_outs, repeat_h), dim=2) # (1, seqlen, hidden*2)\n",
    "        \n",
    "        scores = self.v(torch.tanh(self.W(concat_h))) # (1, seqlen, 1)\n",
    "        probs = torch.softmax(scores, dim=1)\n",
    "        \n",
    "        weighted = enc_outs * probs # (1, seqlen, hidden)\n",
    "        \n",
    "        context = torch.sum(weighted, dim=1, keepdim=False) # (1, hidden)\n",
    "        combined = torch.cat((dec_hidden, context), dim=1)  # (1, hidden*2)\n",
    "        \n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder:\n",
      "AttentionDecoder(\n",
      "  (emb): Embedding(29, 64)\n",
      "  (lstm): LSTMCell(64, 128)\n",
      "  (attn): Attention(\n",
      "    (W): Linear(in_features=256, out_features=100, bias=True)\n",
      "    (v): Linear(in_features=100, out_features=1, bias=False)\n",
      "  )\n",
      "  (clf): Linear(in_features=256, out_features=29, bias=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (objective): CrossEntropyLoss()\n",
      ") \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, lstm_size, attn_size):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        \n",
    "        self.lstm_size = lstm_size\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTMCell(emb_dim, lstm_size)\n",
    "        self.attn = Attention(lstm_size * 2, attn_size)\n",
    "        self.clf = nn.Linear(lstm_size * 2, vocab_size)\n",
    "        \n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.objective = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        \n",
    "    def init_state(self, device):\n",
    "        h_0 = torch.zeros(1, self.lstm_size).to(device)  # (batch, hidden_size)\n",
    "        c_0 = torch.zeros(1, self.lstm_size).to(device)  # (batch, hidden_size)\n",
    "        return h_0, c_0\n",
    "        \n",
    "    def forward(self, enc_outs, targets, curr_token, last_token):\n",
    "        device = enc_outs.device\n",
    "        state = self.init_state(device)\n",
    "        \n",
    "        loss = 0\n",
    "        shifted = targets + [last_token]\n",
    "        for i in range(len(shifted)):\n",
    "            inp = torch.tensor([curr_token]).to(device) # (1,)\n",
    "            \n",
    "            emb = self.emb(inp) # (1, emb_dim)\n",
    "            emb = self.drop(emb)\n",
    "            \n",
    "            state = self.lstm(emb, state)\n",
    "            q_i, _ = state \n",
    "            q_i = self.drop(q_i) # (1, emb_dim)\n",
    "            \n",
    "            combined = self.attn(q_i, enc_outs)\n",
    "            \n",
    "            scores = self.clf(combined)\n",
    "            target = torch.tensor([shifted[i]]).to(device)\n",
    "            loss += self.objective(scores, target)\n",
    "            \n",
    "            curr_token = shifted[i]\n",
    "            \n",
    "        return loss / len(shifted)\n",
    "\n",
    "    def predict(self, enc_outs, curr_token, last_token, maxlen):\n",
    "        device = enc_outs.device\n",
    "        state = self.init_state(device)\n",
    "        \n",
    "        preds = []\n",
    "        for i in range(maxlen):\n",
    "            inp = torch.tensor([curr_token]).to(device)\n",
    "            \n",
    "            emb = self.emb(inp)\n",
    "            \n",
    "            state = self.lstm(emb, state)\n",
    "            q_i, _ = state\n",
    "            \n",
    "            combined = self.attn(q_i, enc_outs)\n",
    "            \n",
    "            scores = self.clf(combined)\n",
    "            pred = torch.argmax(torch.softmax(scores, dim=1))\n",
    "            curr_token = pred\n",
    "            \n",
    "            if last_token == pred:\n",
    "                break\n",
    "                \n",
    "            preds.append(pred)\n",
    "        return preds\n",
    "    \n",
    "    def evaluate(self, enc_outs, targets, curr_token, last_token):\n",
    "        device = enc_outs.device\n",
    "        state = self.init_state(device)\n",
    "        \n",
    "        preds, loss = [], 0\n",
    "        shifted = targets + [last_token]\n",
    "        for i in range(len(shifted)):\n",
    "            inp = torch.tensor([curr_token]).to(device)\n",
    "            \n",
    "            emb = self.emb(inp)\n",
    "            \n",
    "            state = self.lstm(emb, state)\n",
    "            q_i, _ = state\n",
    "            \n",
    "            combined = self.attn(q_i, enc_outs)\n",
    "            \n",
    "            scores = self.clf(combined)\n",
    "            target = torch.tensor([shifted[i]]).to(device)\n",
    "            loss += self.objective(scores, target)\n",
    "            \n",
    "            pred = torch.argmax(torch.softmax(scores, dim=1))\n",
    "            curr_token = pred\n",
    "            \n",
    "            if last_token == pred:\n",
    "                break\n",
    "            preds.append(pred)\n",
    "            \n",
    "        return preds, loss\n",
    "\n",
    "decoder = AttentionDecoder(vocab_size=len(tgt_vocab), \n",
    "                           emb_dim=config['emb_size'], \n",
    "                           lstm_size=config['lstm_size'], \n",
    "                           attn_size=config['attn_size'])\n",
    "saveLogMsg(\"decoder:\\n{}\".format(decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_prediction(sample_preds):\n",
    "    sample_preds = [[tgt_vocab.itos[ix] for ix in each_preds] for each_preds in sample_preds]\n",
    "    sample_preds = [''.join(each_preds) for each_preds in sample_preds]\n",
    "    return sample_preds\n",
    "\n",
    "def predict(encoder, decoder, sample_x, batch_size, pred_size):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(sample_x)):\n",
    "            x = sample_x[i]            \n",
    "            x_preds = decoder.predict(encoder(x), START_IX, STOP_IX, pred_size)\n",
    "            predictions.append(x_preds)\n",
    "    \n",
    "    predictions = map_prediction(predictions)\n",
    "    return predictions\n",
    "\n",
    "def evaluate(encoder, decoder, sample_x, sample_y, batch_size):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    predictions, sample_loss = [], 0.0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(sample_x)):\n",
    "            x = sample_x[i]\n",
    "            y = sample_y[i]\n",
    "            \n",
    "            x_preds, x_loss = decoder.evaluate(encoder(x), y, START_IX, STOP_IX)\n",
    "            predictions.append(x_preds)\n",
    "            sample_loss += x_loss.item()\n",
    "    \n",
    "    sample_loss = sample_loss / len(sample_x) * 1.0\n",
    "    \n",
    "    actuals = map_prediction(sample_y)\n",
    "    predictions = map_prediction(predictions)\n",
    "    \n",
    "    accuracy = accuracy_score(actuals, predictions)\n",
    "    return predictions, sample_loss, accuracy\n",
    "\n",
    "def train(encoder, enc_optim, decoder, dec_optim, train_x, train_y, batch_size):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    train_x, train_y = shuffle(train_x, train_y)\n",
    "\n",
    "    train_loss, batch_loss = 0.0, 0.0\n",
    "    for i in range(len(train_x)):\n",
    "        x = train_x[i]\n",
    "        y = train_y[i]\n",
    "\n",
    "        batch_loss += decoder(encoder(x), y, START_IX, STOP_IX)\n",
    "        \n",
    "        if (i+1) % batch_size or i == len(train_x) - 1:\n",
    "            batch_loss.backward()\n",
    "            enc_optim.step()\n",
    "            dec_optim.step()\n",
    "\n",
    "            encoder.zero_grad(); enc_optim.zero_grad()\n",
    "            decoder.zero_grad(); dec_optim.zero_grad()\n",
    "\n",
    "            train_loss += batch_loss.item()\n",
    "            batch_loss = 0\n",
    "\n",
    "    train_loss = train_loss / len(train_x) * 1.0\n",
    "    \n",
    "    return encoder, decoder, train_x, train_y, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(x, y):\n",
    "    pack = list(zip(x, y))\n",
    "    random.shuffle(pack)\n",
    "    return zip(*pack)\n",
    "\n",
    "def track_best_model(encoder, decoder, epoch, best_acc, valid_acc, valid_loss, patience_track):\n",
    "    if best_acc >= valid_acc:\n",
    "        return best_acc, '', patience_track+1\n",
    "    state = {\n",
    "        'encoder': encoder.state_dict(), \n",
    "        'decoder': decoder.state_dict(),\n",
    "        'acc': valid_acc,\n",
    "        'loss': valid_loss,\n",
    "        'epoch': epoch\n",
    "    }\n",
    "    torch.save(state, config['checkpoint'])\n",
    "    return valid_acc, ' * ', 0\n",
    "\n",
    "def load_best_model():\n",
    "    encoder = Encoder(vocab_size=len(src_vocab), \n",
    "                  emb_dim=config['emb_size'], \n",
    "                  lstm_size=config['lstm_size'], \n",
    "                  z_type=0)\n",
    "    decoder = AttentionDecoder(vocab_size=len(tgt_vocab), \n",
    "                               emb_dim=config['emb_size'], \n",
    "                               lstm_size=config['lstm_size'], \n",
    "                               attn_size=config['attn_size'])\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    state = torch.load(config['checkpoint'], map_location=device)\n",
    "    encoder.load_state_dict(state['encoder'])\n",
    "    decoder.load_state_dict(state['decoder'])\n",
    "    state = {'acc': state['acc'], 'loss': state['loss'], 'epoch': state['epoch']}\n",
    "    return encoder, decoder, state\n",
    "\n",
    "def training_loop(encoder, decoder, train_x, train_y, epochs, batch_size, print_every=1):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    enc_optim = optim.SGD(encoder.parameters(), lr=config['lr'], momentum=config['momentum'])\n",
    "    dec_optim = optim.SGD(decoder.parameters(), lr=config['lr'], momentum=config['momentum'])\n",
    "    \n",
    "    best_acc = -1.0\n",
    "    patience_track = 0\n",
    "    keep_loss = [[], []] # [[train],[valid]]\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        encoder.zero_grad(); enc_optim.zero_grad()\n",
    "        decoder.zero_grad(); dec_optim.zero_grad()\n",
    "\n",
    "        encoder, decoder, train_x, train_y, train_loss = train(encoder, enc_optim, decoder, dec_optim, train_x, train_y, batch_size)\n",
    "        _, valid_loss, valid_acc = evaluate(encoder, decoder, valid_x, valid_y, batch_size)\n",
    "        best_acc, epoch_track, patience_track = track_best_model(encoder, decoder, epoch, best_acc, valid_acc, valid_loss, patience_track)\n",
    "\n",
    "        keep_loss[0].append(train_loss)\n",
    "        keep_loss[1].append(valid_loss)\n",
    "        \n",
    "        if epoch % print_every == 0:\n",
    "            epoch_msg = 'Epoch {} - [TRAIN] Loss: {:.6f}'.format(epoch, train_loss)\n",
    "            epoch_msg += ' [DEV] Loss: {:.6f}, Acc: {:.6f}'.format(valid_loss, valid_acc)\n",
    "            saveLogMsg(epoch_msg + epoch_track)\n",
    "            \n",
    "        if patience_track == int(config['patience']):\n",
    "            saveLogMsg('No accuracy improvment for {} consecutive epochs, stopping training...'.format(config['patience']))\n",
    "            break\n",
    "    \n",
    "    best_encoder, best_decoder, _ = load_best_model()\n",
    "    with open(config['lossfile'], 'wb') as lossfile:\n",
    "        pk.dump(keep_loss, lossfile)\n",
    "    \n",
    "    return best_encoder, best_decoder, keep_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with encoder and decoder... \n",
      "\n",
      "Epoch 1 - [TRAIN] Loss: 0.870242 [DEV] Loss: 10.334611, Acc: 0.712400 *  \n",
      "\n",
      "Epoch 2 - [TRAIN] Loss: 0.305355 [DEV] Loss: 4.578531, Acc: 0.903400 *  \n",
      "\n",
      "Epoch 3 - [TRAIN] Loss: 0.187569 [DEV] Loss: 2.062598, Acc: 0.953000 *  \n",
      "\n",
      "Epoch 4 - [TRAIN] Loss: 0.136223 [DEV] Loss: 0.949622, Acc: 0.980600 *  \n",
      "\n",
      "Epoch 5 - [TRAIN] Loss: 0.117115 [DEV] Loss: 0.686068, Acc: 0.987800 *  \n",
      "\n",
      "Epoch 6 - [TRAIN] Loss: 0.099171 [DEV] Loss: 1.090246, Acc: 0.983600 \n",
      "\n",
      "Epoch 7 - [TRAIN] Loss: 0.094169 [DEV] Loss: 1.729837, Acc: 0.974800 \n",
      "\n",
      "Epoch 8 - [TRAIN] Loss: 0.078536 [DEV] Loss: 0.537171, Acc: 0.992800 *  \n",
      "\n",
      "Epoch 9 - [TRAIN] Loss: 0.073074 [DEV] Loss: 0.336494, Acc: 0.993800 *  \n",
      "\n",
      "Epoch 10 - [TRAIN] Loss: 0.070728 [DEV] Loss: 1.163856, Acc: 0.985200 \n",
      "\n",
      "Epoch 11 - [TRAIN] Loss: 0.064901 [DEV] Loss: 0.785263, Acc: 0.989400 \n",
      "\n",
      "Epoch 12 - [TRAIN] Loss: 0.063445 [DEV] Loss: 0.501394, Acc: 0.994200 *  \n",
      "\n",
      "Epoch 13 - [TRAIN] Loss: 0.061153 [DEV] Loss: 0.312984, Acc: 0.995800 *  \n",
      "\n",
      "Epoch 14 - [TRAIN] Loss: 0.055091 [DEV] Loss: 0.567709, Acc: 0.992600 \n",
      "\n",
      "Epoch 15 - [TRAIN] Loss: 0.055771 [DEV] Loss: 0.429080, Acc: 0.995200 \n",
      "\n",
      "Epoch 16 - [TRAIN] Loss: 0.056700 [DEV] Loss: 0.138078, Acc: 0.997200 *  \n",
      "\n",
      "Epoch 17 - [TRAIN] Loss: 0.059082 [DEV] Loss: 0.646500, Acc: 0.992200 \n",
      "\n",
      "Epoch 18 - [TRAIN] Loss: 0.051418 [DEV] Loss: 1.015856, Acc: 0.987400 \n",
      "\n",
      "Epoch 19 - [TRAIN] Loss: 0.050063 [DEV] Loss: 0.186159, Acc: 0.996800 \n",
      "\n",
      "Epoch 20 - [TRAIN] Loss: 0.049185 [DEV] Loss: 0.323404, Acc: 0.995600 \n",
      "\n",
      "Epoch 21 - [TRAIN] Loss: 0.047547 [DEV] Loss: 0.139091, Acc: 0.997400 *  \n",
      "\n",
      "Epoch 22 - [TRAIN] Loss: 0.049632 [DEV] Loss: 0.371635, Acc: 0.995800 \n",
      "\n",
      "Epoch 23 - [TRAIN] Loss: 0.043019 [DEV] Loss: 0.178121, Acc: 0.998000 *  \n",
      "\n",
      "Epoch 24 - [TRAIN] Loss: 0.042540 [DEV] Loss: 0.219380, Acc: 0.996400 \n",
      "\n",
      "Epoch 25 - [TRAIN] Loss: 0.050628 [DEV] Loss: 0.200715, Acc: 0.997200 \n",
      "\n",
      "Epoch 26 - [TRAIN] Loss: 0.045092 [DEV] Loss: 0.177080, Acc: 0.997000 \n",
      "\n",
      "Epoch 27 - [TRAIN] Loss: 0.047224 [DEV] Loss: 0.116997, Acc: 0.999000 *  \n",
      "\n",
      "Epoch 28 - [TRAIN] Loss: 0.044180 [DEV] Loss: 0.122500, Acc: 0.999000 \n",
      "\n",
      "Epoch 29 - [TRAIN] Loss: 0.043939 [DEV] Loss: 0.414793, Acc: 0.995600 \n",
      "\n",
      "Epoch 30 - [TRAIN] Loss: 0.038646 [DEV] Loss: 0.086939, Acc: 0.999000 \n",
      "\n",
      "Epoch 31 - [TRAIN] Loss: 0.038583 [DEV] Loss: 0.301286, Acc: 0.996000 \n",
      "\n",
      "Epoch 32 - [TRAIN] Loss: 0.040130 [DEV] Loss: 0.219862, Acc: 0.997400 \n",
      "\n",
      "Epoch 33 - [TRAIN] Loss: 0.039854 [DEV] Loss: 0.764612, Acc: 0.991800 \n",
      "\n",
      "Epoch 34 - [TRAIN] Loss: 0.038484 [DEV] Loss: 0.083778, Acc: 0.999200 *  \n",
      "\n",
      "Epoch 35 - [TRAIN] Loss: 0.041153 [DEV] Loss: 0.196506, Acc: 0.997400 \n",
      "\n",
      "Epoch 36 - [TRAIN] Loss: 0.038404 [DEV] Loss: 0.075262, Acc: 0.999000 \n",
      "\n",
      "Epoch 37 - [TRAIN] Loss: 0.039354 [DEV] Loss: 0.198899, Acc: 0.998200 \n",
      "\n",
      "Epoch 38 - [TRAIN] Loss: 0.044518 [DEV] Loss: 0.050999, Acc: 0.999400 *  \n",
      "\n",
      "Epoch 39 - [TRAIN] Loss: 0.040858 [DEV] Loss: 0.305552, Acc: 0.996400 \n",
      "\n",
      "Epoch 40 - [TRAIN] Loss: 0.044783 [DEV] Loss: 0.160398, Acc: 0.998200 \n",
      "\n",
      "Epoch 41 - [TRAIN] Loss: 0.042201 [DEV] Loss: 0.822597, Acc: 0.992200 \n",
      "\n",
      "Epoch 42 - [TRAIN] Loss: 0.040276 [DEV] Loss: 0.254834, Acc: 0.997600 \n",
      "\n",
      "Epoch 43 - [TRAIN] Loss: 0.042149 [DEV] Loss: 0.145543, Acc: 0.998600 \n",
      "\n",
      "Epoch 44 - [TRAIN] Loss: 0.041644 [DEV] Loss: 0.101094, Acc: 0.998800 \n",
      "\n",
      "Epoch 45 - [TRAIN] Loss: 0.041213 [DEV] Loss: 0.108059, Acc: 0.998400 \n",
      "\n",
      "Epoch 46 - [TRAIN] Loss: 0.040222 [DEV] Loss: 0.089928, Acc: 0.998800 \n",
      "\n",
      "Epoch 47 - [TRAIN] Loss: 0.040325 [DEV] Loss: 0.090691, Acc: 0.998600 \n",
      "\n",
      "Epoch 48 - [TRAIN] Loss: 0.042314 [DEV] Loss: 0.038753, Acc: 0.999600 *  \n",
      "\n",
      "Epoch 49 - [TRAIN] Loss: 0.043118 [DEV] Loss: 0.059242, Acc: 0.999200 \n",
      "\n",
      "Epoch 50 - [TRAIN] Loss: 0.044180 [DEV] Loss: 0.474507, Acc: 0.994000 \n",
      "\n",
      "Epoch 51 - [TRAIN] Loss: 0.039659 [DEV] Loss: 0.107321, Acc: 0.998200 \n",
      "\n",
      "Epoch 52 - [TRAIN] Loss: 0.038834 [DEV] Loss: 0.025432, Acc: 0.999600 \n",
      "\n",
      "Epoch 53 - [TRAIN] Loss: 0.039711 [DEV] Loss: 0.101056, Acc: 0.998400 \n",
      "\n",
      "Epoch 54 - [TRAIN] Loss: 0.042169 [DEV] Loss: 0.013759, Acc: 0.999800 *  \n",
      "\n",
      "Epoch 55 - [TRAIN] Loss: 0.040571 [DEV] Loss: 0.139781, Acc: 0.997600 \n",
      "\n",
      "Epoch 56 - [TRAIN] Loss: 0.048132 [DEV] Loss: 0.085083, Acc: 0.998600 \n",
      "\n",
      "Epoch 57 - [TRAIN] Loss: 0.054665 [DEV] Loss: 0.228647, Acc: 0.996600 \n",
      "\n",
      "Epoch 58 - [TRAIN] Loss: 0.049354 [DEV] Loss: 0.070064, Acc: 0.998800 \n",
      "\n",
      "Epoch 59 - [TRAIN] Loss: 0.045847 [DEV] Loss: 0.349442, Acc: 0.995600 \n",
      "\n",
      "Epoch 60 - [TRAIN] Loss: 0.045939 [DEV] Loss: 0.211348, Acc: 0.997400 \n",
      "\n",
      "Epoch 61 - [TRAIN] Loss: 0.048453 [DEV] Loss: 0.179232, Acc: 0.997400 \n",
      "\n",
      "Epoch 62 - [TRAIN] Loss: 0.048045 [DEV] Loss: 0.057757, Acc: 0.998800 \n",
      "\n",
      "Epoch 63 - [TRAIN] Loss: 0.055983 [DEV] Loss: 0.055983, Acc: 0.999400 \n",
      "\n",
      "Epoch 64 - [TRAIN] Loss: 0.050593 [DEV] Loss: 0.061722, Acc: 0.999200 \n",
      "\n",
      "No accuracy improvment for 10 consecutive epochs, stopping training... \n",
      "\n",
      "Training done... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if True: #not os.path.exists(config['checkpoint']):\n",
    "    saveLogMsg(\"Training with encoder and decoder...\")\n",
    "    training_loop(encoder, decoder, train_x, train_y, config['epoch'], config['batch'], print_every=1)\n",
    "    saveLogMsg('Training done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning best model from epoch 54 with loss 0.013759 and accuracy 0.999800. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "keep_loss = [[], []] # [[train],[valid]]\n",
    "if os.path.exists(config['checkpoint']):\n",
    "    with open(config['lossfile'], 'rb') as lossfile:\n",
    "        keep_loss = pk.load(lossfile)\n",
    "    encoder, decoder, state = load_best_model()\n",
    "    saveLogMsg('Returning best model from epoch {} with loss {:.6f} and accuracy {:.6f}.'.format(state['epoch'], state['loss'], state['acc']))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving training and validation loss... \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "saveLogMsg('Retrieving training and validation loss...')\n",
    "\n",
    "# Refs: https://matplotlib.org/tutorials/introductory/pyplot.html\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "epochs = [i for i in range(1, len(keep_loss[0])+1)]\n",
    "plt.plot(epochs, keep_loss[0], 'b', label=\"training loss\")\n",
    "plt.plot(epochs, keep_loss[1], 'g', label=\"validation loss\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('cross-entropy loss')\n",
    "plt.title('The training and validation losses.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset from ../data/raw/test.txt. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "saveLogMsg(\"Loading test dataset from {}.\".format(config['testfile']))\n",
    "\n",
    "test_inp, test_out = [], []\n",
    "with open(config['testfile'], 'r') as testfile:\n",
    "    for eachline in testfile:\n",
    "        eachline = eachline.strip()\n",
    "        if eachline:\n",
    "            eachline = eachline.split()\n",
    "            if len(eachline) == 2:\n",
    "                test_inp.append(eachline[0])\n",
    "                test_out.append(eachline[1])\n",
    "test_x = map_many_elems(test_inp, src_vocab.stoi)\n",
    "test_y = map_many_elems(test_out, tgt_vocab.stoi)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy score = 0.9246 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getAccuracyScore(encoder, decoder, sample_x, sample_out):\n",
    "    predictions = predict(encoder, decoder, sample_x, config['batch'], config['pred_size'])\n",
    "    groundtruth = [''.join(str_y) for str_y in sample_out]\n",
    "    acc = accuracy_score(groundtruth, predictions)\n",
    "    return acc\n",
    "\n",
    "saveLogMsg(\"Test accuracy score = {}\".format(getAccuracyScore(encoder, decoder, test_x, test_out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
