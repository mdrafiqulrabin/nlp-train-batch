{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import _pickle as pk\n",
    "\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "import copy, warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing imports and config... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'num_train': 20000,\n",
    "    'num_valid': 5000,\n",
    "    'patience': 10,\n",
    "    'batch': 32,\n",
    "    'epoch': 3000,\n",
    "    'lr': 0.001,\n",
    "    'momentum': 0.99,\n",
    "    'emb_size': 64,\n",
    "    'lstm_size': 128,\n",
    "    'pred_size': 10,\n",
    "    'logfile': \"model_loop_noattn.log\",\n",
    "    'testfile': \"../data/raw/test.txt\",\n",
    "    'lossfile': 'model_loop_noattn.loss',\n",
    "    'checkpoint': \"model_loop_noattn.pt\"\n",
    "}\n",
    "\n",
    "open(config['logfile'], 'w').close()\n",
    "def saveLogMsg(msg):\n",
    "    print(msg, \"\\n\")\n",
    "    with open(config['logfile'], \"a\") as myfile:\n",
    "        myfile.write(msg + \"\\n\")\n",
    "        \n",
    "saveLogMsg(\"Initializing imports and config...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset for train and valid... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sorting_letters_dataset(size):\n",
    "    dataset = []\n",
    "    for _ in range(size):\n",
    "        x = []\n",
    "        for _ in range(random.randint(3, 10)):\n",
    "            letter = chr(random.randint(97, 122))\n",
    "            repeat = [letter] * random.randint(1, 3)\n",
    "            x.extend(repeat)\n",
    "        y = sorted(set(x))\n",
    "        dataset.append((x, y))\n",
    "    return zip(*dataset)\n",
    "\n",
    "train_inp, train_out = sorting_letters_dataset(config['num_train'])\n",
    "valid_inp, valid_out = sorting_letters_dataset(config['num_valid'])\n",
    "\n",
    "saveLogMsg(\"Dataset for train and valid...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab for source and target... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, vocab):\n",
    "        self.itos = vocab\n",
    "        self.stoi = {d:i for i, d in enumerate(self.itos)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos) \n",
    "\n",
    "src_vocab = Vocab(['<pad>'] + [chr(i+97) for i in range(26)])\n",
    "tgt_vocab = Vocab(['<pad>'] + [chr(i+97) for i in range(26)] + ['<start>', '<stop>'] )\n",
    "\n",
    "START_IX = tgt_vocab.stoi['<start>']\n",
    "STOP_IX  = tgt_vocab.stoi['<stop>']\n",
    "\n",
    "saveLogMsg(\"Vocab for source and target...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping dataset through Vocab... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def map_elems(elems, mapper):\n",
    "    return [mapper[elem] for elem in elems]\n",
    "\n",
    "def map_many_elems(many_elems, mapper):\n",
    "    return [map_elems(elems, mapper) for elems in many_elems]\n",
    "\n",
    "train_x = map_many_elems(train_inp, src_vocab.stoi)\n",
    "train_y = map_many_elems(train_out, tgt_vocab.stoi)\n",
    "\n",
    "valid_x = map_many_elems(valid_inp, src_vocab.stoi)\n",
    "valid_y = map_many_elems(valid_out, tgt_vocab.stoi)\n",
    "\n",
    "saveLogMsg(\"Mapping dataset through Vocab...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder:\n",
      "Encoder(\n",
      "  (emb): Embedding(27, 64)\n",
      "  (lstm): LSTM(64, 128, batch_first=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ") \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, lstm_size, z_type, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.z_index = z_type\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, lstm_size, batch_first=True)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        x_tensor = [torch.tensor(sample).to(device) for sample in inputs]\n",
    "        x_pad = pad_sequence(x_tensor, batch_first=True, padding_value=0) # (batch, seqlen) \n",
    "        x_emb = self.emb(x_pad) # (batch, seqlen, emb_dim) \n",
    "        x_emb = self.drop(x_emb)\n",
    "        \n",
    "        x_len = [len(sample) for sample in inputs]\n",
    "        x_pack = pack_padded_sequence(x_emb, x_len, batch_first=True, enforce_sorted=False)\n",
    "        outs_pack, (h_n, c_n) = self.lstm(x_pack)\n",
    "        outs, _ = pad_packed_sequence(outs_pack, batch_first=True)\n",
    "        \n",
    "        if self.z_index == 1:\n",
    "            return h_n, c_n # (seqlen, batch, lstm_dim)\n",
    "        else:\n",
    "            return outs # (batch, seqlen, lstm_dim)\n",
    "\n",
    "encoder = Encoder(vocab_size=len(src_vocab), \n",
    "                  emb_dim=config['emb_size'], \n",
    "                  lstm_size=config['lstm_size'], \n",
    "                  z_type=1)\n",
    "saveLogMsg(\"encoder:\\n{}\".format(encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder:\n",
      "Decoder(\n",
      "  (emb): Embedding(29, 64)\n",
      "  (lstm): LSTMCell(64, 128)\n",
      "  (clf): Linear(in_features=128, out_features=29, bias=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (objective): CrossEntropyLoss()\n",
      ") \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, lstm_size, dropout=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTMCell(emb_dim, lstm_size)\n",
    "        self.clf = nn.Linear(lstm_size, vocab_size)\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.objective = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        \n",
    "    def forward(self, batch_state, batch_targets, curr_token_raw, last_token_raw):\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        batch_state_h, batch_state_c = batch_state # (seqlen, batch, lstm_dim)\n",
    "        batch_state_ht = batch_state_h.transpose(0, 1) # (batch, seqlen, lstm_dim)\n",
    "        batch_state_ct = batch_state_c.transpose(0, 1) # (batch, seqlen, lstm_dim)\n",
    "        \n",
    "        batch_loss = 0.0\n",
    "        for targets, state_h, state_c in zip(batch_targets, batch_state_ht, batch_state_ct):\n",
    "            curr_token, last_token = curr_token_raw, last_token_raw\n",
    "            state = (state_h, state_c)\n",
    "            shifted = targets + [last_token]\n",
    "            \n",
    "            each_loss = 0.0\n",
    "            for i in range(len(shifted)):\n",
    "                inp = torch.tensor([curr_token]).to(device)\n",
    "\n",
    "                emb = self.emb(inp)\n",
    "                emb = self.drop(emb)\n",
    "\n",
    "                state = self.lstm(emb, state)\n",
    "                q_i, _ = state \n",
    "                q_i = self.drop(q_i)\n",
    "        \n",
    "                scores = self.clf(q_i)\n",
    "                target = torch.tensor([shifted[i]]).to(device)\n",
    "                each_loss += self.objective(scores, target)\n",
    "\n",
    "                curr_token = shifted[i]\n",
    "            \n",
    "            batch_loss += (each_loss / len(shifted) * 1.0)\n",
    "            \n",
    "        return batch_loss\n",
    "\n",
    "    def predict(self, batch_state, curr_token_raw, last_token_raw, maxlen):\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        batch_state_h, batch_state_c = batch_state # (seqlen, batch, lstm_dim)\n",
    "        batch_state_ht = batch_state_h.transpose(0, 1) # (batch, seqlen, lstm_dim)\n",
    "        batch_state_ct = batch_state_c.transpose(0, 1) # (batch, seqlen, lstm_dim)\n",
    "        \n",
    "        batch_preds = []\n",
    "        for state_h, state_c in zip(batch_state_ht, batch_state_ct):\n",
    "            curr_token, last_token = curr_token_raw, last_token_raw\n",
    "            state = (state_h, state_c)\n",
    "            \n",
    "            each_preds = []\n",
    "            for i in range(maxlen):\n",
    "                inp = torch.tensor([curr_token]).to(device)\n",
    "                \n",
    "                emb = self.emb(inp)\n",
    "\n",
    "                state = self.lstm(emb, state)\n",
    "                h_i, _ = state\n",
    "\n",
    "                scores = self.clf(h_i)\n",
    "                pred = torch.argmax(torch.softmax(scores, dim=1))\n",
    "                curr_token = pred\n",
    "\n",
    "                if last_token == pred:\n",
    "                    break\n",
    "                each_preds.append(pred)\n",
    "                \n",
    "            batch_preds.append(each_preds)\n",
    "            \n",
    "        return batch_preds\n",
    "    \n",
    "    def evaluate(self, batch_state, batch_targets, curr_token_raw, last_token_raw):\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        batch_state_h, batch_state_c = batch_state # (seqlen, batch, lstm_dim)\n",
    "        batch_state_ht = batch_state_h.transpose(0, 1) # (batch, seqlen, lstm_dim)\n",
    "        batch_state_ct = batch_state_c.transpose(0, 1) # (batch, seqlen, lstm_dim)\n",
    "        \n",
    "        batch_preds = []\n",
    "        batch_loss = 0.0\n",
    "        for state_h, state_c, targets in zip(batch_state_ht, batch_state_ct, batch_targets):\n",
    "            curr_token, last_token = curr_token_raw, last_token_raw\n",
    "            state = (state_h, state_c)\n",
    "            shifted = targets + [last_token]\n",
    "            \n",
    "            each_preds = []\n",
    "            each_loss = 0.0\n",
    "            for i in range(len(shifted)):\n",
    "                inp = torch.tensor([curr_token]).to(device)\n",
    "                \n",
    "                emb = self.emb(inp)\n",
    "\n",
    "                state = self.lstm(emb, state)\n",
    "                h_i, _ = state\n",
    "\n",
    "                scores = self.clf(h_i)\n",
    "                target = torch.tensor([shifted[i]]).to(device)\n",
    "                each_loss += self.objective(scores, target)\n",
    "                \n",
    "                pred = torch.argmax(torch.softmax(scores, dim=1))\n",
    "                curr_token = pred\n",
    "\n",
    "                if last_token == pred:\n",
    "                    break\n",
    "                each_preds.append(pred)\n",
    "                \n",
    "            batch_loss += (each_loss / len(each_preds) * 1.0)\n",
    "            batch_preds.append(each_preds)\n",
    "            \n",
    "        return batch_preds, batch_loss\n",
    "\n",
    "decoder = Decoder(vocab_size=len(tgt_vocab), \n",
    "                  emb_dim=config['emb_size'], \n",
    "                  lstm_size=config['lstm_size'])\n",
    "saveLogMsg(\"decoder:\\n{}\".format(decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_prediction(sample_preds):\n",
    "    sample_preds = [[tgt_vocab.itos[ix] for ix in each_preds] for each_preds in sample_preds]\n",
    "    sample_preds = [''.join(each_preds) for each_preds in sample_preds]\n",
    "    return sample_preds\n",
    "\n",
    "def predict(encoder, decoder, sample_x, batch_size, pred_size):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    batch_x = []\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(sample_x)):\n",
    "            batch_x.append(sample_x[i])\n",
    "            \n",
    "            if len(batch_x) == batch_size or i == len(sample_x) - 1:\n",
    "                batch_preds = decoder.predict(encoder(batch_x), START_IX, STOP_IX, pred_size)\n",
    "                batch_preds = map_prediction(batch_preds)\n",
    "                predictions.extend(batch_preds)\n",
    "                batch_x = []\n",
    "            \n",
    "    return predictions\n",
    "\n",
    "def evaluate(encoder, decoder, sample_x, sample_y, batch_size):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    sample_loss = 0.0\n",
    "    batch_x, batch_y = [], []\n",
    "    predictions, actuals = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(sample_x)):\n",
    "            batch_x.append(sample_x[i])\n",
    "            batch_y.append(sample_y[i])\n",
    "            \n",
    "            if len(batch_x) == batch_size or i == len(sample_x) - 1:\n",
    "                batch_preds, batch_loss = decoder.evaluate(encoder(batch_x), batch_y, START_IX, STOP_IX)\n",
    "                \n",
    "                batch_preds = map_prediction(batch_preds)\n",
    "                predictions.extend(batch_preds)\n",
    "                batch_y = map_prediction(batch_y)\n",
    "                actuals.extend(batch_y)\n",
    "                \n",
    "                sample_loss += batch_loss.item()\n",
    "                batch_x, batch_y = [], []\n",
    "    \n",
    "    sample_loss = sample_loss / len(sample_x) * 1.0\n",
    "    \n",
    "    accuracy = accuracy_score(actuals, predictions)\n",
    "    return predictions, sample_loss, accuracy\n",
    "\n",
    "def train(encoder, enc_optim, decoder, dec_optim, train_x, train_y, batch_size):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    train_x, train_y = shuffle(train_x, train_y)\n",
    "    batch_x, batch_y = [], []\n",
    "\n",
    "    for i in range(len(train_x)):\n",
    "        batch_x.append(train_x[i])\n",
    "        batch_y.append(train_y[i])\n",
    "\n",
    "        if len(batch_x) == batch_size or i == len(train_x) - 1:\n",
    "            batch_loss = decoder(encoder(batch_x), batch_y, START_IX, STOP_IX)\n",
    "\n",
    "            batch_loss.backward()\n",
    "            enc_optim.step()\n",
    "            dec_optim.step()\n",
    "\n",
    "            encoder.zero_grad(); enc_optim.zero_grad()\n",
    "            decoder.zero_grad(); dec_optim.zero_grad()\n",
    "\n",
    "            train_loss += batch_loss.item()\n",
    "            batch_x, batch_y = [], []\n",
    "\n",
    "    train_loss = train_loss / len(train_x) * 1.0\n",
    "    \n",
    "    return encoder, decoder, train_x, train_y, train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(x, y):\n",
    "    pack = list(zip(x, y))\n",
    "    random.shuffle(pack)\n",
    "    return zip(*pack)\n",
    "\n",
    "def track_best_model(encoder, decoder, epoch, best_acc, valid_acc, valid_loss, patience_track):\n",
    "    if best_acc >= valid_acc:\n",
    "        return best_acc, '', patience_track+1\n",
    "    state = {\n",
    "        'encoder': encoder.state_dict(), \n",
    "        'decoder': decoder.state_dict(),\n",
    "        'acc': valid_acc,\n",
    "        'loss': valid_loss,\n",
    "        'epoch': epoch\n",
    "    }\n",
    "    torch.save(state, config['checkpoint'])\n",
    "    return valid_acc, ' * ', 0\n",
    "\n",
    "def load_best_model():\n",
    "    encoder = Encoder(vocab_size=len(src_vocab), \n",
    "                  emb_dim=config['emb_size'], \n",
    "                  lstm_size=config['lstm_size'], \n",
    "                  z_type=1)\n",
    "    decoder = Decoder(vocab_size=len(tgt_vocab), \n",
    "                  emb_dim=config['emb_size'], \n",
    "                  lstm_size=config['lstm_size'])\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    state = torch.load(config['checkpoint'], map_location=device)\n",
    "    encoder.load_state_dict(state['encoder'])\n",
    "    decoder.load_state_dict(state['decoder'])\n",
    "    state = {'acc': state['acc'], 'loss': state['loss'], 'epoch': state['epoch']}\n",
    "    return encoder, decoder, state\n",
    "\n",
    "def training_loop(encoder, decoder, train_x, train_y, epochs, batch_size, print_every=1):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    enc_optim = optim.SGD(encoder.parameters(), lr=config['lr'], momentum=config['momentum'])\n",
    "    dec_optim = optim.SGD(decoder.parameters(), lr=config['lr'], momentum=config['momentum'])\n",
    "    \n",
    "    best_acc = -1.0\n",
    "    patience_track = 0\n",
    "    keep_loss = [[], []] # [[train],[valid]]\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        encoder.zero_grad(); enc_optim.zero_grad()\n",
    "        decoder.zero_grad(); dec_optim.zero_grad()\n",
    "        \n",
    "        encoder, decoder, train_x, train_y, train_loss = train(encoder, enc_optim, decoder, dec_optim, train_x, train_y, batch_size)\n",
    "        _, valid_loss, valid_acc = evaluate(encoder, decoder, valid_x, valid_y, batch_size)\n",
    "        best_acc, epoch_track, patience_track = track_best_model(encoder, decoder, epoch, best_acc, valid_acc, valid_loss, patience_track)\n",
    "        \n",
    "        keep_loss[0].append(train_loss)\n",
    "        keep_loss[1].append(valid_loss)\n",
    "        \n",
    "        if epoch % print_every == 0:\n",
    "            epoch_msg = 'Epoch {} - [TRAIN] Loss: {:.6f}'.format(epoch, train_loss)\n",
    "            epoch_msg += ' [DEV] Loss: {:.6f}, Acc: {:.6f}'.format(valid_loss, valid_acc)\n",
    "            saveLogMsg(epoch_msg + epoch_track)\n",
    "            \n",
    "        if patience_track == int(config['patience']):\n",
    "            saveLogMsg('No accuracy improvment for {} consecutive epochs, stopping training...'.format(config['patience']))\n",
    "            break\n",
    "    \n",
    "    best_encoder, best_decoder, _ = load_best_model()\n",
    "    with open(config['lossfile'], 'wb') as lossfile:\n",
    "        pk.dump(keep_loss, lossfile)\n",
    "    \n",
    "    return best_encoder, best_decoder, keep_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with encoder and decoder... \n",
      "\n",
      "Epoch 1 - [TRAIN] Loss: 1.844442 [DEV] Loss: 1.485004, Acc: 0.275800 *  \n",
      "\n",
      "Epoch 2 - [TRAIN] Loss: 0.725624 [DEV] Loss: 0.566061, Acc: 0.741200 *  \n",
      "\n",
      "Epoch 3 - [TRAIN] Loss: 0.378644 [DEV] Loss: 0.213370, Acc: 0.904600 *  \n",
      "\n",
      "Epoch 4 - [TRAIN] Loss: 0.213595 [DEV] Loss: 0.071513, Acc: 0.965200 *  \n",
      "\n",
      "Epoch 5 - [TRAIN] Loss: 0.142425 [DEV] Loss: 0.038960, Acc: 0.982200 *  \n",
      "\n",
      "Epoch 6 - [TRAIN] Loss: 0.099749 [DEV] Loss: 0.037703, Acc: 0.984200 *  \n",
      "\n",
      "Epoch 7 - [TRAIN] Loss: 0.079309 [DEV] Loss: 0.019547, Acc: 0.990800 *  \n",
      "\n",
      "Epoch 8 - [TRAIN] Loss: 0.068503 [DEV] Loss: 0.036061, Acc: 0.987600 \n",
      "\n",
      "Epoch 9 - [TRAIN] Loss: 0.059796 [DEV] Loss: 0.018907, Acc: 0.991600 *  \n",
      "\n",
      "Epoch 10 - [TRAIN] Loss: 0.052725 [DEV] Loss: 0.022814, Acc: 0.991000 \n",
      "\n",
      "Epoch 11 - [TRAIN] Loss: 0.043876 [DEV] Loss: 0.017002, Acc: 0.992400 *  \n",
      "\n",
      "Epoch 12 - [TRAIN] Loss: 0.040587 [DEV] Loss: 0.009461, Acc: 0.995800 *  \n",
      "\n",
      "Epoch 13 - [TRAIN] Loss: 0.045954 [DEV] Loss: 0.013554, Acc: 0.994600 \n",
      "\n",
      "Epoch 14 - [TRAIN] Loss: 0.043888 [DEV] Loss: 0.009532, Acc: 0.995800 \n",
      "\n",
      "Epoch 15 - [TRAIN] Loss: 0.038539 [DEV] Loss: 0.009627, Acc: 0.996000 *  \n",
      "\n",
      "Epoch 16 - [TRAIN] Loss: 0.030844 [DEV] Loss: 0.008944, Acc: 0.995800 \n",
      "\n",
      "Epoch 17 - [TRAIN] Loss: 0.033330 [DEV] Loss: 0.007047, Acc: 0.996400 *  \n",
      "\n",
      "Epoch 18 - [TRAIN] Loss: 0.032098 [DEV] Loss: 0.007199, Acc: 0.996600 *  \n",
      "\n",
      "Epoch 19 - [TRAIN] Loss: 0.032012 [DEV] Loss: 0.005814, Acc: 0.997200 *  \n",
      "\n",
      "Epoch 20 - [TRAIN] Loss: 0.031263 [DEV] Loss: 0.006209, Acc: 0.996800 \n",
      "\n",
      "Epoch 21 - [TRAIN] Loss: 0.029391 [DEV] Loss: 0.010409, Acc: 0.995800 \n",
      "\n",
      "Epoch 22 - [TRAIN] Loss: 0.029328 [DEV] Loss: 0.007088, Acc: 0.997800 *  \n",
      "\n",
      "Epoch 23 - [TRAIN] Loss: 0.026238 [DEV] Loss: 0.014339, Acc: 0.995600 \n",
      "\n",
      "Epoch 24 - [TRAIN] Loss: 0.026828 [DEV] Loss: 0.011082, Acc: 0.996200 \n",
      "\n",
      "Epoch 25 - [TRAIN] Loss: 0.026350 [DEV] Loss: 0.005692, Acc: 0.997200 \n",
      "\n",
      "Epoch 26 - [TRAIN] Loss: 0.028726 [DEV] Loss: 0.029375, Acc: 0.994800 \n",
      "\n",
      "Epoch 27 - [TRAIN] Loss: 0.025519 [DEV] Loss: 0.007524, Acc: 0.997400 \n",
      "\n",
      "Epoch 28 - [TRAIN] Loss: 0.024239 [DEV] Loss: 0.009593, Acc: 0.996200 \n",
      "\n",
      "Epoch 29 - [TRAIN] Loss: 0.021670 [DEV] Loss: 0.006797, Acc: 0.997200 \n",
      "\n",
      "Epoch 30 - [TRAIN] Loss: 0.024913 [DEV] Loss: 0.005351, Acc: 0.997200 \n",
      "\n",
      "Epoch 31 - [TRAIN] Loss: 0.023364 [DEV] Loss: 0.008263, Acc: 0.996200 \n",
      "\n",
      "Epoch 32 - [TRAIN] Loss: 0.028131 [DEV] Loss: 0.017069, Acc: 0.995000 \n",
      "\n",
      "No accuracy improvment for 10 consecutive epochs, stopping training... \n",
      "\n",
      "Training done... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "keep_loss = [[], []] # [[train],[valid]]\n",
    "if not os.path.exists(config['checkpoint']):\n",
    "    saveLogMsg(\"Training with encoder and decoder...\")\n",
    "    encoder, decoder, keep_loss = training_loop(encoder, decoder, train_x, train_y, config['epoch'], config['batch'], print_every=1)\n",
    "    saveLogMsg('Training done...')\n",
    "else:\n",
    "    with open(config['lossfile'], 'rb') as lossfile:\n",
    "        keep_loss = pk.load(lossfile)\n",
    "    encoder, decoder, state = load_best_model()\n",
    "    saveLogMsg('Returning best model from epoch {} with loss {:.6f} and accuracy {:.6f}.'.format(state['epoch'], state['loss'], state['acc']))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving training and validation loss... \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "saveLogMsg('Retrieving training and validation loss...')\n",
    "\n",
    "keep_loss = [[], []] # [[train],[valid]]\n",
    "with open(config['lossfile'], 'rb') as lossfile:\n",
    "    keep_loss = pk.load(lossfile)\n",
    "\n",
    "# Refs: https://matplotlib.org/tutorials/introductory/pyplot.html\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "epochs = [i for i in range(1, len(keep_loss[0])+1)]\n",
    "plt.plot(epochs, keep_loss[0], 'b', label=\"training loss\")\n",
    "plt.plot(epochs, keep_loss[1], 'g', label=\"validation loss\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('cross-entropy loss')\n",
    "plt.title('The training and validation losses.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset from ../data/raw/test.txt. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "saveLogMsg(\"Loading test dataset from {}.\".format(config['testfile']))\n",
    "\n",
    "test_inp, test_out = [], []\n",
    "with open(config['testfile'], 'r') as testfile:\n",
    "    for eachline in testfile:\n",
    "        eachline = eachline.strip()\n",
    "        if eachline:\n",
    "            eachline = eachline.split()\n",
    "            if len(eachline) == 2:\n",
    "                test_inp.append(eachline[0])\n",
    "                test_out.append(eachline[1])\n",
    "test_x = map_many_elems(test_inp, src_vocab.stoi)\n",
    "test_y = map_many_elems(test_out, tgt_vocab.stoi)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy score = 0.9184 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getAccuracyScore(encoder, decoder, sample_x, sample_out):\n",
    "    predictions = predict(encoder, decoder, sample_x, config['batch'], config['pred_size'])\n",
    "    groundtruth = [''.join(str_y) for str_y in sample_out]\n",
    "    acc = accuracy_score(groundtruth, predictions)\n",
    "    return acc\n",
    "\n",
    "saveLogMsg(\"Test accuracy score = {}\".format(getAccuracyScore(encoder, decoder, test_x, test_out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
