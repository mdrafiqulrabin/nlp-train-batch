{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'num_train': 200,\n",
    "    'num_valid': 50,\n",
    "    'batch': 5,\n",
    "    'epoch': 50,\n",
    "    'patient': 20,\n",
    "    'lr': 0.001,\n",
    "    'momentum': 0.99,\n",
    "    'encoder_emb_size': 64,\n",
    "    'decoder_emb_size': 64,\n",
    "    'lstm_size': 128,\n",
    "    'max_pred': 10,\n",
    "    'logfile': \"lstm-batch-loop.log\",\n",
    "    'checkpoint': \"seq2seq-loop.pt\"\n",
    "}\n",
    "\n",
    "open(config['logfile'], 'w').close()\n",
    "def saveLogMsg(msg):\n",
    "    print(msg, \"\\n\")\n",
    "    with open(config['logfile'], \"a\") as myfile:\n",
    "        myfile.write(msg + \"\\n\")\n",
    "saveLogMsg(\"Starting...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_letters_dataset(size):\n",
    "    dataset = []\n",
    "    for _ in range(size):\n",
    "        x = []\n",
    "        for _ in range(random.randint(3, 10)):\n",
    "            letter = chr(random.randint(97, 122))\n",
    "            repeat = [letter] * random.randint(1, 3)\n",
    "            x.extend(repeat)\n",
    "        y = sorted(set(x))\n",
    "        dataset.append((x, y))\n",
    "    return zip(*dataset)\n",
    "\n",
    "train_inp, train_out = sorting_letters_dataset(config['num_train'])\n",
    "valid_inp, valid_out = sorting_letters_dataset(config['num_valid'])\n",
    "\n",
    "saveLogMsg(\"Dataset for train and valid...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, vocab):\n",
    "        self.itos = vocab\n",
    "        self.stoi = {d:i for i, d in enumerate(self.itos)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos) \n",
    "\n",
    "src_vocab = Vocab(['<pad>'] + [chr(i+97) for i in range(26)])\n",
    "tgt_vocab = Vocab(['<pad>'] + [chr(i+97) for i in range(26)] + ['<start>', '<stop>'] )\n",
    "\n",
    "START_IX = tgt_vocab.stoi['<start>']\n",
    "STOP_IX  = tgt_vocab.stoi['<stop>']\n",
    "\n",
    "saveLogMsg(\"Vocab for source and target...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_elems(elems, mapper):\n",
    "    return [mapper[elem] for elem in elems]\n",
    "\n",
    "def map_many_elems(many_elems, mapper):\n",
    "    return [map_elems(elems, mapper) for elems in many_elems]\n",
    "\n",
    "train_x = map_many_elems(train_inp, src_vocab.stoi)\n",
    "train_y = map_many_elems(train_out, tgt_vocab.stoi)\n",
    "\n",
    "valid_x = map_many_elems(valid_inp, src_vocab.stoi)\n",
    "valid_y = map_many_elems(valid_out, tgt_vocab.stoi)\n",
    "\n",
    "saveLogMsg(\"Mapping dataset through Vocab...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, lstm_size, z_type, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.z_index = z_type\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, lstm_size, batch_first=True)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        x_tensor = [torch.tensor(sample).to(device) for sample in inputs]\n",
    "        x_pad = pad_sequence(x_tensor, batch_first=True, padding_value=0) # (batch, seqlen) \n",
    "        x_emb = self.emb(x_pad) # (batch, seqlen, emb_dim) \n",
    "        x_emb = self.drop(x_emb)\n",
    "        \n",
    "        x_len = [len(sample) for sample in inputs]\n",
    "        x_pack = pack_padded_sequence(x_emb, x_len, batch_first=True, enforce_sorted=False)\n",
    "        outs_pack, (h_n, c_n) = self.lstm(x_pack)\n",
    "        outs, _ = pad_packed_sequence(outs_pack, batch_first=True)\n",
    "        \n",
    "        if self.z_index == 1:\n",
    "            return h_n, c_n # (seqlen, lstm_dim)\n",
    "        else:\n",
    "            return outs # (1, seqlen, lstm_dim)\n",
    "\n",
    "encoder = Encoder(vocab_size=len(src_vocab), \n",
    "                  emb_dim=config['encoder_emb_size'], \n",
    "                  lstm_size=config['lstm_size'], \n",
    "                  z_type=1)\n",
    "saveLogMsg(\"encoder:\\n{}\".format(encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, lstm_size, dropout=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTMCell(emb_dim, lstm_size)\n",
    "        self.clf = nn.Linear(lstm_size, vocab_size)\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.objective = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        \n",
    "    def forward(self, batch_state, batch_targets, curr_token_raw, last_token_raw):\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        batch_state_h, batch_state_c = batch_state\n",
    "        batch_state_ht = batch_state_h.transpose(0, 1)\n",
    "        batch_state_ct = batch_state_c.transpose(0, 1)\n",
    "        \n",
    "        batch_loss = 0.0\n",
    "        for targets, state_h, state_c in zip(batch_targets, batch_state_ht, batch_state_ct):\n",
    "            curr_token, last_token = curr_token_raw, last_token_raw\n",
    "            state = (state_h, state_c)\n",
    "            shifted = targets + [last_token]\n",
    "            \n",
    "            each_loss = 0.0\n",
    "            for i in range(len(shifted)):\n",
    "                inp = torch.tensor([curr_token]).to(device)\n",
    "\n",
    "                emb = self.emb(inp)\n",
    "                emb = self.drop(emb)\n",
    "\n",
    "                state = self.lstm(emb, state)\n",
    "                q_i, _ = state \n",
    "                q_i = self.drop(q_i)\n",
    "\n",
    "                scores = self.clf(q_i)\n",
    "                target = torch.tensor([shifted[i]]).to(device)\n",
    "                each_loss += self.objective(scores, target)\n",
    "\n",
    "                curr_token = shifted[i]\n",
    "            \n",
    "            batch_loss += (each_loss / len(shifted) * 1.0)\n",
    "            \n",
    "        return batch_loss # / len(targets)\n",
    "\n",
    "    def predict(self, batch_state, batch_targets, curr_token_raw, last_token_raw):\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        batch_state_h, batch_state_c = batch_state\n",
    "        batch_state_ht = batch_state_h.transpose(0, 1)\n",
    "        batch_state_ct = batch_state_c.transpose(0, 1)\n",
    "        \n",
    "        batch_preds = []\n",
    "        batch_loss = 0.0\n",
    "        for state_h, state_c, targets in zip(batch_state_ht, batch_state_ct, batch_targets):\n",
    "            curr_token, last_token = curr_token_raw, last_token_raw\n",
    "            state = (state_h, state_c)\n",
    "            \n",
    "            each_preds = []\n",
    "            each_loss = 0.0\n",
    "            for i in range(maxlen):\n",
    "                inp = torch.tensor([curr_token]).to(device)\n",
    "                \n",
    "                emb = self.emb(inp)\n",
    "\n",
    "                state = self.lstm(emb, state)\n",
    "                h_i, _ = state\n",
    "\n",
    "                scores = self.clf(h_i)\n",
    "                target = torch.tensor([?]).to(device)\n",
    "                each_loss += self.objective(scores, target)\n",
    "                \n",
    "                pred = torch.argmax(torch.softmax(scores, dim=1))\n",
    "                curr_token = pred\n",
    "\n",
    "                if last_token == pred:\n",
    "                    break\n",
    "                each_preds.append(pred)\n",
    "            batch_preds.append(each_preds)\n",
    "        return batch_preds\n",
    "    \n",
    "decoder = Decoder(vocab_size=len(tgt_vocab), \n",
    "                  emb_dim=config['decoder_emb_size'], \n",
    "                  lstm_size=config['lstm_size'])\n",
    "saveLogMsg(\"decoder:\\n{}\".format(decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(x, y):\n",
    "    pack = list(zip(x, y))\n",
    "    random.shuffle(pack)\n",
    "    return zip(*pack)\n",
    "\n",
    "def track_best_model(model_path, model, epoch, best_acc, dev_acc, dev_loss, patient_track):\n",
    "    if best_acc > dev_acc:\n",
    "        return best_acc, '', patient_track+1\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'acc': dev_acc,\n",
    "        'loss': dev_loss,\n",
    "        'model': model.state_dict()\n",
    "    }\n",
    "    torch.save(state, model_path)\n",
    "    return dev_acc, ' * ', 0\n",
    "\n",
    "def evaluate(encoder, decoder, sample_x, sample_y, batch_size):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    batch_x, batch_y = [], []\n",
    "    predictions, actuals = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(sample_x)):\n",
    "            batch_x.append(sample_x[i])\n",
    "            batch_y.append(sample_y[i])\n",
    "            \n",
    "            if len(batch_x) == batch_size or i == len(sample_x) - 1:\n",
    "                actuals.extend(batch_y)\n",
    "                batch_preds = decoder.predict(encoder(batch_x), START_IX, STOP_IX, maxlen=config['max_pred'])\n",
    "                batch_preds = [[tgt_vocab.itos[ix] for ix in each_preds] for each_preds in batch_preds]\n",
    "                batch_preds = [''.join(each_preds) for each_preds in batch_preds]\n",
    "                predictions.extend(batch_preds)\n",
    "                batch_x, batch_y = [], []\n",
    "    \n",
    "    return y_preds\n",
    "\n",
    "def train(encoder, decoder, train_x, train_y, batch_size=50, epochs=10, print_every=1):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    enc_optim = optim.SGD(encoder.parameters(), lr=config['lr'], momentum=config['momentum'])\n",
    "    dec_optim = optim.SGD(decoder.parameters(), lr=config['lr'], momentum=config['momentum'])\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        encoder.zero_grad(); enc_optim.zero_grad()\n",
    "        decoder.zero_grad(); dec_optim.zero_grad()\n",
    "\n",
    "        train_x, train_y = shuffle(train_x, train_y)\n",
    "        batch_x, batch_y = [], []\n",
    "\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i in range(len(train_x)):\n",
    "            batch_x.append(train_x[i])\n",
    "            batch_y.append(train_y[i])\n",
    "\n",
    "            if len(batch_x) == batch_size or i == len(train_x) - 1:\n",
    "                batch_loss = decoder(encoder(batch_x), batch_y, START_IX, STOP_IX)\n",
    "            \n",
    "                batch_loss.backward()\n",
    "                enc_optim.step()\n",
    "                dec_optim.step()\n",
    "\n",
    "                encoder.zero_grad(); enc_optim.zero_grad()\n",
    "                decoder.zero_grad(); dec_optim.zero_grad()\n",
    "\n",
    "                epoch_loss += batch_loss.item()\n",
    "                batch_x, batch_y = [], []\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            saveLogMsg(f\"**** Epoch {epoch} - Loss: {epoch_loss / len(train_x):.6f} ****\")\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveLogMsg(\"Training with encoder and decoder...\")\n",
    "encoder, decoder = train(encoder, decoder, \n",
    "                         train_x, train_y, \n",
    "                         batch_size=config['batch'], epochs=config['epoch'], \n",
    "                         print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'encoder': encoder.state_dict(), 'decoder': decoder.state_dict()}, config['checkpoint'])\n",
    "saveLogMsg(\"Saved model as {}...\".format(config['checkpoint']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(encoder, decoder, valid_x, tgt_vocab.itos, batch_size=16)\n",
    "groundtruth = [''.join(t) for t in valid_out]\n",
    "\n",
    "accs = accuracy_score(groundtruth, predictions)\n",
    "saveLogMsg(\"accuracy_score = {}...\".format(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
